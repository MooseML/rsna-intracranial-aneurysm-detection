{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "1. **DICOM → 3D Volume**: Normalize to `(32, 384, 384)`\n",
    "2. **maxvit_base_tf_384**: 32-channel input, 14 binary outputs\n",
    "3. **Ensemble**: Average 5-fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T12:10:14.875544Z",
     "iopub.status.busy": "2025-09-22T12:10:14.874935Z",
     "iopub.status.idle": "2025-09-22T12:10:14.879267Z",
     "shell.execute_reply": "2025-09-22T12:10:14.878564Z",
     "shell.execute_reply.started": "2025-09-22T12:10:14.875518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import timm\n",
    "# print(\"timm version:\", timm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T12:10:15.248194Z",
     "iopub.status.busy": "2025-09-22T12:10:15.247958Z",
     "iopub.status.idle": "2025-09-22T12:10:15.252129Z",
     "shell.execute_reply": "2025-09-22T12:10:15.251346Z",
     "shell.execute_reply.started": "2025-09-22T12:10:15.248173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import timm, pprint\n",
    "\n",
    "# # all models (long list)\n",
    "# all_models = timm.list_models()\n",
    "# print(len(all_models), \"models available\")\n",
    "# for pat in [\n",
    "#     \"*convnextv2*\",\n",
    "#     \"*efficientnetv2*\",\n",
    "#     \"*maxvit*\",\n",
    "#     \"*regnet*\",\n",
    "#     \"*vit_*patch16*\",\n",
    "#     \"*swin*\",\n",
    "# ]:\n",
    "#     print(\"\\n\", pat)\n",
    "#     pprint.pp(sorted(timm.list_models(pat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-22T12:32:04.743235Z",
     "iopub.status.busy": "2025-09-22T12:32:04.742737Z",
     "iopub.status.idle": "2025-09-22T12:32:04.770084Z",
     "shell.execute_reply": "2025-09-22T12:32:04.769413Z",
     "shell.execute_reply.started": "2025-09-22T12:32:04.743211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from scipy import ndimage\n",
    "import warnings\n",
    "import gc\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DICOMPreprocessorKaggle:\n",
    "    \"\"\"\n",
    "    DICOM preprocessing that converts original \n",
    "    DICOMPreprocessor logic to single series processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_shape: Tuple[int, int, int] = (32, 384, 384)):\n",
    "        self.target_depth, self.target_height, self.target_width = target_shape\n",
    "        \n",
    "    def load_dicom_series(self, series_path: str) -> Tuple[List[pydicom.Dataset], str]:\n",
    "        \"\"\"\n",
    "        Load DICOM series\n",
    "        \"\"\"\n",
    "        series_path = Path(series_path)\n",
    "        series_name = series_path.name\n",
    "        \n",
    "        # Search for DICOM files\n",
    "        dicom_files = []\n",
    "        for root, _, files in os.walk(series_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.dcm'):\n",
    "                    dicom_files.append(os.path.join(root, file))\n",
    "        \n",
    "        if not dicom_files:\n",
    "            raise ValueError(f\"No DICOM files found in {series_path}\")\n",
    "        \n",
    "        #print(f\"Found {len(dicom_files)} DICOM files in series {series_name}\")\n",
    "        \n",
    "        # Load DICOM datasets\n",
    "        datasets = []\n",
    "        for filepath in dicom_files:\n",
    "            try:\n",
    "                ds = pydicom.dcmread(filepath, force=True)\n",
    "                datasets.append(ds)\n",
    "            except Exception as e:\n",
    "                #print(f\"Failed to load {filepath}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not datasets:\n",
    "            raise ValueError(f\"No valid DICOM files in {series_path}\")\n",
    "        \n",
    "        return datasets, series_name\n",
    "    \n",
    "    def extract_slice_info(self, datasets: List[pydicom.Dataset]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract position information for each slice\n",
    "        \"\"\"\n",
    "        slice_info = []\n",
    "        \n",
    "        for i, ds in enumerate(datasets):\n",
    "            info = {\n",
    "                'dataset': ds,\n",
    "                'index': i,\n",
    "                'instance_number': getattr(ds, 'InstanceNumber', i),\n",
    "            }\n",
    "            \n",
    "            # Get z-coordinate from ImagePositionPatient\n",
    "            try:\n",
    "                position = getattr(ds, 'ImagePositionPatient', None)\n",
    "                if position is not None and len(position) >= 3:\n",
    "                    info['z_position'] = float(position[2])\n",
    "                else:\n",
    "                    # Fallback: use InstanceNumber\n",
    "                    info['z_position'] = float(info['instance_number'])\n",
    "                    #print(\"ImagePositionPatient not found, using InstanceNumber\")\n",
    "            except Exception as e:\n",
    "                info['z_position'] = float(i)\n",
    "                #print(f\"Failed to extract position info: {e}\")\n",
    "            \n",
    "            slice_info.append(info)\n",
    "        \n",
    "        return slice_info\n",
    "    \n",
    "    def sort_slices_by_position(self, slice_info: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Sort slices by z-coordinate\n",
    "        \"\"\"\n",
    "        # Sort by z-coordinate\n",
    "        sorted_slices = sorted(slice_info, key=lambda x: x['z_position'])\n",
    "        \n",
    "        #print(f\"Sorted {len(sorted_slices)} slices by z-position\")\n",
    "        #print(f\"Z-range: {sorted_slices[0]['z_position']:.2f} to {sorted_slices[-1]['z_position']:.2f}\")\n",
    "        \n",
    "        return sorted_slices\n",
    "    \n",
    "    def get_windowing_params(self, ds: pydicom.Dataset, img: np.ndarray = None) -> Tuple[Optional[float], Optional[float]]:\n",
    "        \"\"\"\n",
    "        Return (center, width) for windowing if appropriate, else (None, None).\n",
    "        For CTA/CT we use a fixed angiography window; for MR we skip windowing.\n",
    "        \"\"\"\n",
    "        modality = str(getattr(ds, \"Modality\", \"CT\")).upper()\n",
    "    \n",
    "        if modality == \"CT\":\n",
    "            # CTA-style windowing for vessels\n",
    "            center, width = 50.0, 350.0\n",
    "            return center, width\n",
    "    \n",
    "        # MR and other modalities: do percentile-based normalization downstream\n",
    "        return None, None\n",
    "\n",
    "    \n",
    "    def apply_windowing_or_normalize(self, img: np.ndarray, center: Optional[float], width: Optional[float]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        If (center,width) provided -> apply window to 0-255.\n",
    "        Else -> robust percentile normalization to 0-255.\n",
    "        Returns uint8.\n",
    "        \"\"\"\n",
    "        if center is not None and width is not None:\n",
    "            # CT/CTA windowing\n",
    "            img_min = center - width / 2.0\n",
    "            img_max = center + width / 2.0\n",
    "            windowed = np.clip(img, img_min, img_max)\n",
    "            windowed = (windowed - img_min) / max(1e-6, (img_max - img_min))\n",
    "            return (windowed * 255.0).astype(np.uint8)\n",
    "    \n",
    "        # MR (or unknown) -> percentile normalization\n",
    "        p1, p99 = np.percentile(img, [1, 99])\n",
    "        if p99 > p1:\n",
    "            norm = np.clip(img, p1, p99)\n",
    "            norm = (norm - p1) / (p99 - p1)\n",
    "            return (norm * 255.0).astype(np.uint8)\n",
    "    \n",
    "        # Fallback: min-max\n",
    "        mn, mx = float(img.min()), float(img.max())\n",
    "        if mx > mn:\n",
    "            norm = (img - mn) / (mx - mn)\n",
    "            return (norm * 255.0).astype(np.uint8)\n",
    "        return np.zeros_like(img, dtype=np.uint8)\n",
    "\n",
    "    \n",
    "    def extract_pixel_array(self, ds: pydicom.Dataset) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract 2D pixel array from DICOM and apply basic preprocessing.\n",
    "        Returns float32 image (no scaling to 0–255 here).\n",
    "        \"\"\"\n",
    "        # Raw pixel data to float32\n",
    "        img = ds.pixel_array.astype(np.float32)\n",
    "    \n",
    "        # If multi-frame (3D in a single file), take the middle frame for 2D path\n",
    "        if img.ndim == 3:\n",
    "            frame_idx = img.shape[0] // 2\n",
    "            img = img[frame_idx]\n",
    "    \n",
    "        # Handle MONOCHROME1 (invert): larger values are darker -> flip\n",
    "        if getattr(ds, \"PhotometricInterpretation\", \"\").upper() == \"MONOCHROME1\":\n",
    "            # Invert relative to full range to preserve dynamic range\n",
    "            img = img.max() - img\n",
    "    \n",
    "        # Apply DICOM rescale (DO NOT override slope/intercept)\n",
    "        slope = float(getattr(ds, \"RescaleSlope\", 1.0))\n",
    "        intercept = float(getattr(ds, \"RescaleIntercept\", 0.0))\n",
    "        if slope != 1.0 or intercept != 0.0:\n",
    "            img = img * slope + intercept\n",
    "    \n",
    "        # Optional: mask out pixel padding value if present (common in CT)\n",
    "        if hasattr(ds, \"PixelPaddingValue\"):\n",
    "            ppv = float(ds.PixelPaddingValue)\n",
    "            img = np.where(np.isclose(img, ppv), np.nan, img)\n",
    "    \n",
    "        # Replace NaNs introduced by padding with local minimum (keeps dtype)\n",
    "        if np.isnan(img).any():\n",
    "            # Use finite min; if all NaN (shouldn't happen), fill zeros\n",
    "            finite = img[np.isfinite(img)]\n",
    "            fill_val = finite.min() if finite.size else 0.0\n",
    "            img = np.nan_to_num(img, nan=fill_val)\n",
    "    \n",
    "        return img  # float32\n",
    "\n",
    "    \n",
    "    def resize_volume_3d(self, volume: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Resize 3D volume to target size\n",
    "        \"\"\"\n",
    "        current_shape = volume.shape\n",
    "        target_shape = (self.target_depth, self.target_height, self.target_width)\n",
    "        \n",
    "        if current_shape == target_shape:\n",
    "            return volume\n",
    "        \n",
    "        #print(f\"Resizing volume from {current_shape} to {target_shape}\")\n",
    "        \n",
    "        # 3D resizing using scipy.ndimage\n",
    "        zoom_factors = [\n",
    "            target_shape[i] / current_shape[i] for i in range(3)\n",
    "        ]\n",
    "        \n",
    "        # Resize with linear interpolation\n",
    "        resized_volume = ndimage.zoom(volume, zoom_factors, order=1, mode='nearest')\n",
    "        \n",
    "        # Clip to exact size just in case\n",
    "        resized_volume = resized_volume[:self.target_depth, :self.target_height, :self.target_width]\n",
    "        \n",
    "        # Padding if necessary\n",
    "        pad_width = [\n",
    "            (0, max(0, self.target_depth - resized_volume.shape[0])),\n",
    "            (0, max(0, self.target_height - resized_volume.shape[1])),\n",
    "            (0, max(0, self.target_width - resized_volume.shape[2]))\n",
    "        ]\n",
    "        \n",
    "        if any(pw[1] > 0 for pw in pad_width):\n",
    "            resized_volume = np.pad(resized_volume, pad_width, mode='edge')\n",
    "        \n",
    "        #print(f\"Final volume shape: {resized_volume.shape}\")\n",
    "        return resized_volume.astype(np.uint8)\n",
    "    \n",
    "    def process_series(self, series_path: str, return_meta: bool = False):\n",
    "        \"\"\"\n",
    "        Process DICOM series and return resampled NumPy array.\n",
    "        If return_meta=True, also returns a dict with:\n",
    "            - 'orig_depth': int, number of source slices/frames\n",
    "            - 'spacing':   tuple (dz, dy, dx) if available, else None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1) Load DICOM files (headers + pixels on demand)\n",
    "            datasets, series_name = self.load_dicom_series(series_path)\n",
    "    \n",
    "            # --- compute orig_depth from headers, BEFORE resampling ---\n",
    "            orig_depth = None\n",
    "            dz = dy = dx = None\n",
    "    \n",
    "            if len(datasets) == 1:\n",
    "                ds0 = datasets[0]\n",
    "                # multiframe (enhanced) 3D?\n",
    "                nframes = getattr(ds0, \"NumberOfFrames\", None)\n",
    "                if nframes is not None:\n",
    "                    try:\n",
    "                        orig_depth = int(nframes)\n",
    "                    except Exception:\n",
    "                        orig_depth = None\n",
    "                # if not set, try pixel array ndim==3\n",
    "                if orig_depth is None:\n",
    "                    try:\n",
    "                        arr0 = ds0.pixel_array  # pydicom will decode\n",
    "                        if arr0.ndim == 3:\n",
    "                            orig_depth = int(arr0.shape[0])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            else:\n",
    "                # multiple single-slice files: count unique z positions if possible\n",
    "                z_vals = []\n",
    "                for ds in datasets:\n",
    "                    ipp = getattr(ds, \"ImagePositionPatient\", None)\n",
    "                    iop = getattr(ds, \"ImageOrientationPatient\", None)\n",
    "                    if ipp is not None and iop is not None and len(ipp) == 3 and len(iop) >= 6:\n",
    "                        # Quick proxy: use z = IPP[2]\n",
    "                        z_vals.append(float(ipp[2]))\n",
    "                    else:\n",
    "                        sl = getattr(ds, \"SliceLocation\", None)\n",
    "                        if sl is not None:\n",
    "                            z_vals.append(float(sl))\n",
    "                if z_vals:\n",
    "                    orig_depth = int(np.unique(np.round(z_vals, 5)).size)\n",
    "                if orig_depth is None:\n",
    "                    # fallback: number of DICOM files\n",
    "                    orig_depth = len(datasets)\n",
    "    \n",
    "            # optional spacing (best-effort)\n",
    "            try:\n",
    "                # dy, dx from PixelSpacing; dz from SpacingBetweenSlices or SliceThickness\n",
    "                ds_ref = datasets[0]\n",
    "                px = getattr(ds_ref, \"PixelSpacing\", None)  # [dy, dx]\n",
    "                dy = float(px[0]) if px is not None else None\n",
    "                dx = float(px[1]) if px is not None else None\n",
    "                dz = getattr(ds_ref, \"SpacingBetweenSlices\", None)\n",
    "                dz = float(dz) if dz is not None else None\n",
    "                if dz is None:\n",
    "                    st = getattr(ds_ref, \"SliceThickness\", None)\n",
    "                    dz = float(st) if st is not None else None\n",
    "            except Exception:\n",
    "                dz = dy = dx = None\n",
    "    \n",
    "            # 2) Produce the resampled volume (your existing logic)\n",
    "            first_ds = datasets[0]\n",
    "            first_img = first_ds.pixel_array\n",
    "    \n",
    "            if len(datasets) == 1 and first_img.ndim == 3:\n",
    "                vol = self._process_single_3d_dicom(first_ds, series_name)  # (32,H,W) float/uint8\n",
    "            else:\n",
    "                vol = self._process_multiple_2d_dicoms(datasets, series_name)  # (32,H,W)\n",
    "    \n",
    "            if return_meta:\n",
    "                return vol, {\"orig_depth\": int(orig_depth) if orig_depth is not None else None,\n",
    "                             \"spacing\": (dz, dy, dx) if (dz is not None and dy is not None and dx is not None) else None}\n",
    "            return vol\n",
    "    \n",
    "        except Exception:\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def _process_single_3d_dicom(self, ds: pydicom.Dataset, series_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Process single 3D DICOM file (for Kaggle: no file saving)\n",
    "        \"\"\"\n",
    "        # Get pixel array\n",
    "        volume = ds.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Apply RescaleSlope and RescaleIntercept\n",
    "        slope = float(getattr(ds, \"RescaleSlope\", 1.0))\n",
    "        intercept = float(getattr(ds, \"RescaleIntercept\", 0.0))\n",
    "        if slope != 1.0 or intercept != 0.0:\n",
    "            volume = volume * slope + intercept\n",
    "            # #print(f\"Applied rescaling: slope={slope}, intercept={intercept}\")\n",
    "        \n",
    "        # Get windowing settings\n",
    "        window_center, window_width = self.get_windowing_params(ds)\n",
    "        \n",
    "        # Apply windowing to each slice\n",
    "        processed_slices = []\n",
    "        for i in range(volume.shape[0]):\n",
    "            slice_img = volume[i]\n",
    "            processed_img = self.apply_windowing_or_normalize(slice_img, window_center, window_width)\n",
    "            processed_slices.append(processed_img)\n",
    "        \n",
    "        volume = np.stack(processed_slices, axis=0)\n",
    "        ##print(f\"3D volume shape after windowing: {volume.shape}\")\n",
    "        \n",
    "        # 3D resize\n",
    "        final_volume = self.resize_volume_3d(volume)\n",
    "        \n",
    "        ##print(f\"Successfully processed 3D DICOM series {series_name}\")\n",
    "        return final_volume\n",
    "    \n",
    "    def _process_multiple_2d_dicoms(self, datasets: List[pydicom.Dataset], series_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Process multiple 2D DICOM files (for Kaggle: no file saving)\n",
    "        \"\"\"\n",
    "        slice_info = self.extract_slice_info(datasets)\n",
    "        sorted_slices = self.sort_slices_by_position(slice_info)\n",
    "        first_img = self.extract_pixel_array(sorted_slices[0]['dataset'])\n",
    "        window_center, window_width = self.get_windowing_params(sorted_slices[0]['dataset'], first_img)\n",
    "        processed_slices = []\n",
    "        \n",
    "        for slice_data in sorted_slices:\n",
    "            ds = slice_data['dataset']\n",
    "            img = self.extract_pixel_array(ds)\n",
    "            processed_img = self.apply_windowing_or_normalize(img, window_center, window_width)\n",
    "            # resized_img = cv2.resize(processed_img, (self.target_width, self.target_height), interpolation=cv2.INTER_AREA)\n",
    "            resized_img = cv2.resize(processed_img, (self.target_width, self.target_height), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            processed_slices.append(resized_img)\n",
    "\n",
    "        volume = np.stack(processed_slices, axis=0)\n",
    "        ##print(f\"2D slices stacked to volume shape: {volume.shape}\")\n",
    "        final_volume = self.resize_volume_3d(volume)\n",
    "        \n",
    "        ##print(f\"Successfully processed 2D DICOM series {series_name}\")\n",
    "        return final_volume\n",
    "\n",
    "def process_dicom_series_kaggle(series_path: str, target_shape: Tuple[int, int, int] = (32, 384, 384)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    DICOM processing function for Kaggle inference (single series)\n",
    "    \n",
    "    Args:\n",
    "        series_path: Path to DICOM series\n",
    "        target_shape: Target volume size (depth, height, width)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Processed volume\n",
    "    \"\"\"\n",
    "    preprocessor = DICOMPreprocessorKaggle(target_shape=target_shape)\n",
    "    return preprocessor.process_series(series_path)\n",
    "\n",
    "# Safe processing function with memory cleanup\n",
    "def process_dicom_series_safe(series_path: str, target_shape: Tuple[int, int, int] = (32, 384, 384)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe DICOM processing with memory cleanup\n",
    "    \n",
    "    Args:\n",
    "        series_path: Path to DICOM series\n",
    "        target_shape: Target volume size (depth, height, width)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Processed volume\n",
    "    \"\"\"\n",
    "    try:\n",
    "        volume = process_dicom_series_kaggle(series_path, target_shape)\n",
    "        return volume\n",
    "    finally:\n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "\n",
    "# Test function\n",
    "def test_single_series(series_path: str, target_shape: Tuple[int, int, int] = (32, 384, 384)):\n",
    "    \"\"\"\n",
    "    Test processing for single series\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #print(f\"Testing single series: {series_path}\")\n",
    "        \n",
    "        # Execute processing\n",
    "        volume = process_dicom_series_safe(series_path, target_shape)\n",
    "        \n",
    "        # Display results\n",
    "        #print(f\"✓ Successfully processed series\")\n",
    "        #print(f\"  Volume shape: {volume.shape}\")\n",
    "        #print(f\"  Volume dtype: {volume.dtype}\")\n",
    "        #print(f\"  Volume range: [{volume.min()}, {volume.max()}]\")\n",
    "        \n",
    "        return volume\n",
    "        \n",
    "    except Exception as e:\n",
    "        #print(f\"✗ Failed to process series: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T12:32:06.564097Z",
     "iopub.status.busy": "2025-09-22T12:32:06.563821Z",
     "iopub.status.idle": "2025-09-22T12:32:06.571478Z",
     "shell.execute_reply": "2025-09-22T12:32:06.570865Z",
     "shell.execute_reply.started": "2025-09-22T12:32:06.564078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  DDP / AMP helpers\n",
    "\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "def get_dist_env():\n",
    "    \"\"\"Read torchrun/torch.distributed env with safe defaults.\"\"\"\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", os.environ.get(\"SLURM_LOCALID\", 0)))\n",
    "    rank       = int(os.environ.get(\"RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    return local_rank, rank, world_size\n",
    "\n",
    "def setup_distributed():\n",
    "    local_rank, rank, world_size = get_dist_env()\n",
    "    is_distributed = world_size > 1\n",
    "    if is_distributed and not dist.is_initialized():\n",
    "        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    return local_rank, rank, world_size, is_distributed\n",
    "\n",
    "def cleanup_distributed():\n",
    "    if dist.is_available() and dist.is_initialized():\n",
    "        dist.barrier()\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "def is_main_process():\n",
    "    return int(os.environ.get(\"RANK\", 0)) == 0\n",
    "\n",
    "def seed_everything(base_seed=42):\n",
    "    # different seed per rank for true shuffling; still reproducible\n",
    "    _, rank, _ = get_dist_env()\n",
    "    seed = base_seed + rank\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    return seed\n",
    "\n",
    "def scale_lr_for_world_size(lr: float):\n",
    "    \"\"\"Linear LR scaling (per-GPU batch fixed, total batch = world_size * perGPU).\"\"\"\n",
    "    _, _, world_size = get_dist_env()\n",
    "    return lr * max(1, world_size)\n",
    "\n",
    "# Optional toggles you can use elsewhere\n",
    "USE_CHANNELS_LAST = True     # improves throughput on T4 with AMP\n",
    "USE_TORCH_COMPILE = False     # try torch.compile; fall back if it errors\n",
    "AMP_DTYPE = \"bf16\"           # T4 → fp16; (A100/H100 can use \"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T12:32:09.345585Z",
     "iopub.status.busy": "2025-09-22T12:32:09.345005Z",
     "iopub.status.idle": "2025-09-22T12:32:09.357030Z",
     "shell.execute_reply": "2025-09-22T12:32:09.356410Z",
     "shell.execute_reply.started": "2025-09-22T12:32:09.345559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Config & labels ===\n",
    "import os, math, json, random, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "\n",
    "# Label order must match competition columns\n",
    "LABEL_COLS = [\n",
    "    \"Left Infraclinoid Internal Carotid Artery\",\n",
    "    \"Right Infraclinoid Internal Carotid Artery\",\n",
    "    \"Left Supraclinoid Internal Carotid Artery\",\n",
    "    \"Right Supraclinoid Internal Carotid Artery\",\n",
    "    \"Left Middle Cerebral Artery\",\n",
    "    \"Right Middle Cerebral Artery\",\n",
    "    \"Anterior Communicating Artery\",\n",
    "    \"Left Anterior Cerebral Artery\",\n",
    "    \"Right Anterior Cerebral Artery\",\n",
    "    \"Left Posterior Communicating Artery\",\n",
    "    \"Right Posterior Communicating Artery\",\n",
    "    \"Basilar Tip\",\n",
    "    \"Other Posterior Circulation\",\n",
    "    \"Aneurysm Present\",\n",
    "]\n",
    "ANEURYSM_PRESENT_IDX = 13\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    series_root: str = r\"D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/series\"\n",
    "    train_csv: str  = r\"D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/train.csv\"\n",
    "    localizers_csv_path: str = r\"D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/train_localizers.csv\"\n",
    "\n",
    "    img_size: int = 384\n",
    "    base_slices: int = 32\n",
    "    extra_cached_chans: int = 0\n",
    "    use_vessel_sidecar: bool = True\n",
    "    vessel_sidecar_mode: str = \"mip\"\n",
    "    vessel_extra: int = 1 if (use_vessel_sidecar and vessel_sidecar_mode == \"mip\") else (32 if (use_vessel_sidecar and vessel_sidecar_mode == \"per_slice\") else 0)\n",
    "    use_localizers: bool = True\n",
    "    max_localizer_crops: int = 3\n",
    "    \n",
    "    local_crop_size: int = 128\n",
    "    p_localizer_dropout: float = 0.30\n",
    "    # optional: occasionally turn localizers fully off\n",
    "    p_global_localizer_off: float = 0.10\n",
    "\n",
    "    num_classes: int = 14\n",
    "    model_name: str = \"maxvit_base_tf_384\"\n",
    "    epochs: int = 34\n",
    "    batch_size: int = 2\n",
    "    num_workers: int = 2\n",
    "    lr: float = 1.6e-4\n",
    "    weight_decay: float = 0.05\n",
    "    warmup_epochs: float = 5.0\n",
    "    min_lr: float = 3e-6\n",
    "    clip_grad_norm: float = 1.0\n",
    "    use_amp: bool = True\n",
    "    label_smoothing: float = 0.02\n",
    "    focal_loss: bool = False\n",
    "    focal_gamma: float = 1.5\n",
    "    pos_weight: float = 1.0\n",
    "    folds: int = 5\n",
    "    seed: int = 42\n",
    "    out_dir: str = \"./outputs\"\n",
    "    save_name: str = \"maxvitbasemodel\"\n",
    "    seeds: list = field(default_factory=lambda: [42, 2025, 8])\n",
    "\n",
    "    # computed after init\n",
    "    in_chans: int = base_slices + extra_cached_chans + max_localizer_crops + vessel_extra \n",
    "    # in_chans: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        k = self.max_localizer_crops if self.use_localizers else 0\n",
    "        self.in_chans = self.base_slices + self.extra_cached_chans + k\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "def per_label_auc(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    for i, name in enumerate(LABEL_COLS):\n",
    "        yi, pi = y_true[:, i], y_prob[:, i]\n",
    "        out[name] = roc_auc_score(yi, pi) if len(np.unique(yi)) >= 2 else np.nan\n",
    "    return out\n",
    "\n",
    "def comp_weighted_auc(aucs: Dict[str, float]) -> float:\n",
    "    weights, vals = [], []\n",
    "    for i, name in enumerate(LABEL_COLS):\n",
    "        w = 13.0 if i == ANEURYSM_PRESENT_IDX else 1.0\n",
    "        if not np.isnan(aucs[name]):\n",
    "            weights.append(w); vals.append(aucs[name]*w)\n",
    "    return (sum(vals)/sum(weights)) if weights else np.nan\n",
    "\n",
    "def cfg_to_dict(cfg_cls) -> dict:\n",
    "    return {\n",
    "        k: getattr(cfg_cls, k) \n",
    "        for k in dir(cfg_cls)\n",
    "        if not k.startswith(\"__\") and not callable(getattr(cfg_cls, k))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cacheing via shards\n",
    "Strategy\n",
    "\n",
    "Shard the cache into folders/files ≤ ~5–8 GB each (safe margin).\n",
    "\n",
    "In each Kaggle run, build one shard in /kaggle/working/shard_k/…, then:\n",
    "\n",
    "Option A: leave as plain .npy files inside shard folder.\n",
    "\n",
    "Option B (nice for scale): pack into WebDataset tar shards (.tar with simple naming).\n",
    "\n",
    "Download the shard (or “Commit & Save Output”) and upload as a Kaggle Dataset (either one dataset with many files or multiple versions).\n",
    "\n",
    "In your training notebook, Add Data → select your dataset(s). They’ll appear under /kaggle/input/<your-dataset>/… (no 20 GB limit).\n",
    "\n",
    "Your DataLoader reads from /kaggle/input paths (mmap .npy or stream tar shards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T12:32:10.136697Z",
     "iopub.status.busy": "2025-09-22T12:32:10.136448Z",
     "iopub.status.idle": "2025-09-22T12:32:12.107189Z",
     "shell.execute_reply": "2025-09-22T12:32:12.106303Z",
     "shell.execute_reply.started": "2025-09-22T12:32:10.136680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import os, math, hashlib, numpy as np, pandas as pd, time, multiprocessing as mp\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # Prevent oversubscription (each worker will do BLAS/ndimage work)\n",
    "# os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# IMG_SIZE      = CFG.img_size      # 384 to preserve quality (or 320)\n",
    "# BYTES_PER_ELT = 1                 # uint8\n",
    "# DEPTH         = 32\n",
    "# TARGET_BYTES_PER_SHARD = 6 * 1024**3\n",
    "# OUT_BASE = f\"/kaggle/working/cache_u8_{IMG_SIZE}\"\n",
    "\n",
    "# # Choose which shard to build in THIS run:\n",
    "# NUM_SHARDS   = None   # None = auto-compute\n",
    "# SHARD_ID     = 3      # change per run\n",
    "# NUM_WORKERS  = max(1, mp.cpu_count()-1)  \n",
    "\n",
    "# df_all = pd.read_csv(CFG.train_csv)\n",
    "# df_all = df_all[df_all[\"SeriesInstanceUID\"].apply(\n",
    "#     lambda u: os.path.isdir(os.path.join(CFG.series_root, str(u)))\n",
    "# )].reset_index(drop=True)\n",
    "\n",
    "# bytes_per_series = DEPTH * IMG_SIZE * IMG_SIZE * BYTES_PER_ELT\n",
    "# est_total_bytes  = len(df_all) * bytes_per_series\n",
    "# if NUM_SHARDS is None:\n",
    "#     NUM_SHARDS = max(1, math.ceil(est_total_bytes / TARGET_BYTES_PER_SHARD))\n",
    "\n",
    "# print(f\"[Shard plan] ~{est_total_bytes/1e9:.2f} GB total, \"\n",
    "#       f\"~{TARGET_BYTES_PER_SHARD/1e9:.1f} GB per shard → NUM_SHARDS={NUM_SHARDS}\")\n",
    "# assert 0 <= SHARD_ID < NUM_SHARDS, \"Set SHARD_ID within [0, NUM_SHARDS)\"\n",
    "\n",
    "# def sid_to_shard(sid: str, num_shards: int) -> int:\n",
    "#     h = int(hashlib.md5(sid.encode(\"utf-8\")).hexdigest()[:8], 16)\n",
    "#     return h % num_shards\n",
    "\n",
    "# df_shard = df_all[df_all[\"SeriesInstanceUID\"].astype(str).map(\n",
    "#     lambda s: sid_to_shard(s, NUM_SHARDS) == SHARD_ID\n",
    "# )].reset_index(drop=True)\n",
    "# print(f\"[Shard {SHARD_ID}/{NUM_SHARDS}] series: {len(df_shard)}\")\n",
    "\n",
    "# OUT_DIR = f\"{OUT_BASE}_shard{SHARD_ID:02d}\"\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# print(\"Output dir:\", OUT_DIR)\n",
    "\n",
    "# def cache_path_for(sid: str) -> str:\n",
    "#     return os.path.join(OUT_DIR, f\"{sid}_{IMG_SIZE}.npy\")\n",
    "\n",
    "# sids_all = df_shard[\"SeriesInstanceUID\"].astype(str).tolist()\n",
    "# to_write = [sid for sid in sids_all if not os.path.exists(cache_path_for(sid))]\n",
    "# to_skip  = len(sids_all) - len(to_write)\n",
    "# print(f\"Will save: {len(to_write)}  |  Already cached (skip): {to_skip}\")\n",
    "\n",
    "# # Worker function (creates its own preprocessor)\n",
    "# def _worker(sid):\n",
    "#     try:\n",
    "#         from __main__ import DICOMPreprocessorKaggle, CFG\n",
    "#         preproc = DICOMPreprocessorKaggle(target_shape=(DEPTH, IMG_SIZE, IMG_SIZE))\n",
    "#         vol, meta = preproc.process_series(os.path.join(CFG.series_root, sid), return_meta=True)\n",
    "#         if vol.dtype != np.uint8:\n",
    "#             vol = np.clip(vol, 0, 255).astype(np.uint8)\n",
    "#         np.save(cache_path_for(sid), vol, allow_pickle=False)\n",
    "#         return (sid, int(meta.get('orig_depth') or 32), True, None)\n",
    "#     except Exception as e:\n",
    "#         return (sid, None, False, str(e))\n",
    "\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Collect results from workers\n",
    "# results = []\n",
    "# ok, err = 0, 0\n",
    "# with mp.Pool(processes=NUM_WORKERS) as pool, tqdm(total=len(to_write), unit=\"series\",\n",
    "#                                                   desc=f\"Shard {SHARD_ID} @ {IMG_SIZE}px\") as pbar:\n",
    "#     for sid, orig_depth, success, error_msg in pool.imap_unordered(_worker, to_write, chunksize=2):\n",
    "#         results.append((sid, orig_depth, success, error_msg))\n",
    "#         if success: ok += 1\n",
    "#         else: err += 1\n",
    "#         pbar.update(1)\n",
    "\n",
    "# # Build manifest rows: include both processed and skipped\n",
    "# rows = []\n",
    "\n",
    "# # 1) Add processed results\n",
    "# for sid, orig_depth, success, error_msg in results:\n",
    "#     rows.append({\n",
    "#         \"SeriesInstanceUID\": sid,\n",
    "#         \"img_size\": IMG_SIZE,\n",
    "#         \"cache_path\": cache_path_for(sid),\n",
    "#         \"orig_depth\": orig_depth,\n",
    "#         \"skipped\": False,\n",
    "#         \"success\": bool(success),\n",
    "#         \"error\": (None if success else (str(error_msg) if error_msg is not None else \"unknown\"))\n",
    "#     })\n",
    "\n",
    "# # 2) Add skipped (already cached) entries so the manifest is complete\n",
    "# skipped_sids = [sid for sid in sids_all if sid not in set(to_write)]\n",
    "# for sid in skipped_sids:\n",
    "#     rows.append({\n",
    "#         \"SeriesInstanceUID\": sid,\n",
    "#         \"img_size\": IMG_SIZE,\n",
    "#         \"cache_path\": cache_path_for(sid),\n",
    "#         \"orig_depth\": None,   # unknown because we didn't reprocess; can be filled later if you have it\n",
    "#         \"skipped\": True,\n",
    "#         \"success\": True,\n",
    "#         \"error\": None\n",
    "#     })\n",
    "\n",
    "# # Save manifest as Parquet in the shard folder\n",
    "# manifest_path = os.path.join(OUT_DIR, f\"manifest_{IMG_SIZE}.parquet\")\n",
    "# pd.DataFrame(rows).to_parquet(manifest_path, index=False)\n",
    "# print(f\"Manifest saved: {manifest_path}\")\n",
    "\n",
    "# dt = time.time() - t0\n",
    "# print(f\"[Shard {SHARD_ID}] saved: {ok}, skipped: {to_skip}, errors: {err}, \"\n",
    "#       f\"elapsed: {dt/60:.1f} min (~{(dt/max(1, max(ok,1))):0.2f}s/series)\")\n",
    "# print(\"Shard folder is ready to download or save as Notebook Output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check preprocessing between cached images and raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T12:32:12.312907Z",
     "iopub.status.busy": "2025-09-22T12:32:12.312679Z",
     "iopub.status.idle": "2025-09-22T12:35:05.153071Z",
     "shell.execute_reply": "2025-09-22T12:35:05.152497Z",
     "shell.execute_reply.started": "2025-09-22T12:32:12.312889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # =============================\n",
    "# # Cache vs Raw Preprocessing Check\n",
    "# # =============================\n",
    "# import os, random, math, numpy as np, pandas as pd\n",
    "# from pathlib import Path\n",
    "# from typing import List, Optional, Tuple\n",
    "# from tqdm.auto import tqdm\n",
    "# import glob\n",
    "\n",
    "# # --- reuse your cache discovery helpers ---\n",
    "# def discover_shard_roots_for(img_size: int) -> List[str]:\n",
    "#     SHARDS_ROOT = \"/kaggle/input/shards\"\n",
    "#     pattern = os.path.join(SHARDS_ROOT, \"*\", f\"cache_u8_{img_size}_shard*\")\n",
    "#     roots = sorted([p for p in glob.glob(pattern) if os.path.isdir(p)])\n",
    "#     if is_main_process():\n",
    "#         print(f\"Found {len(roots)} shard roots for {img_size}px\")\n",
    "#     return roots\n",
    "\n",
    "# def make_find_cached_path(shard_roots: List[str]):\n",
    "#     def _find_cached_path(sid: str, img_size: int) -> Optional[str]:\n",
    "#         fname = f\"{sid}_{img_size}.npy\"\n",
    "#         for root in shard_roots:\n",
    "#             p = os.path.join(root, fname)\n",
    "#             if os.path.exists(p):\n",
    "#                 return p\n",
    "#         return None\n",
    "#     return _find_cached_path\n",
    "\n",
    "# def _psnr_u8(a: np.ndarray, b: np.ndarray) -> float:\n",
    "#     # a, b uint8 arrays of same shape\n",
    "#     diff = a.astype(np.float32) - b.astype(np.float32)\n",
    "#     mse = float(np.mean(diff**2))\n",
    "#     if mse == 0: \n",
    "#         return float('inf')\n",
    "#     return 20.0 * math.log10(255.0) - 10.0 * math.log10(mse)\n",
    "\n",
    "# def _compare_pair(vol_cached_u8: np.ndarray, vol_raw_u8: np.ndarray) -> dict:\n",
    "#     assert vol_cached_u8.shape == vol_raw_u8.shape, f\"Shape mismatch {vol_cached_u8.shape} vs {vol_raw_u8.shape}\"\n",
    "#     assert vol_cached_u8.dtype == np.uint8 and vol_raw_u8.dtype == np.uint8\n",
    "\n",
    "#     a = vol_cached_u8\n",
    "#     b = vol_raw_u8\n",
    "\n",
    "#     mean_abs = float(np.mean(np.abs(a.astype(np.int16) - b.astype(np.int16))))\n",
    "#     max_abs  = int(np.max(np.abs(a.astype(np.int16) - b.astype(np.int16))))\n",
    "#     eq_rate  = float(np.mean(a == b))\n",
    "#     psnr     = _psnr_u8(a, b)\n",
    "\n",
    "#     # also compare after scaling to [0,1] like training input\n",
    "#     a01 = a.astype(np.float32) / 255.0\n",
    "#     b01 = b.astype(np.float32) / 255.0\n",
    "#     mae01 = float(np.mean(np.abs(a01 - b01)))\n",
    "#     rmse01 = float(np.sqrt(np.mean((a01 - b01)**2)))\n",
    "\n",
    "#     return {\n",
    "#         \"mean_abs_u8\": mean_abs,\n",
    "#         \"max_abs_u8\":  max_abs,\n",
    "#         \"eq_rate\":     eq_rate,\n",
    "#         \"psnr_u8\":     psnr,\n",
    "#         \"mae_01\":      mae01,\n",
    "#         \"rmse_01\":     rmse01,\n",
    "#         \"min_cached\":  int(a.min()), \"max_cached\": int(a.max()),\n",
    "#         \"min_raw\":     int(b.min()), \"max_raw\":   int(b.max()),\n",
    "#     }\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def verify_cache_vs_raw(sample_n: int = 50, seed: int = 123, verbose: bool = True) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Compare cached u8 volumes vs on-the-fly preprocessor outputs.\n",
    "#     Returns a DataFrame with metrics per sampled SID.\n",
    "#     \"\"\"\n",
    "#     rng = random.Random(seed)\n",
    "\n",
    "#     # load list of available series\n",
    "#     df_all = pd.read_csv(CFG.train_csv)\n",
    "#     exists = df_all[\"SeriesInstanceUID\"].apply(lambda u: os.path.isdir(os.path.join(CFG.series_root, str(u))))\n",
    "#     df_all = df_all[exists].reset_index(drop=True)\n",
    "\n",
    "#     # cache resolver\n",
    "#     shard_roots = discover_shard_roots_for(CFG.img_size)\n",
    "#     find_cached_path = make_find_cached_path(shard_roots)\n",
    "\n",
    "#     # pick SIDs that have a cache file\n",
    "#     candidates = []\n",
    "#     for sid in df_all[\"SeriesInstanceUID\"].astype(str).tolist():\n",
    "#         cp = find_cached_path(sid, CFG.img_size)\n",
    "#         if cp is not None:\n",
    "#             candidates.append((sid, cp))\n",
    "\n",
    "#     if len(candidates) == 0:\n",
    "#         raise RuntimeError(\"No cached files found; build cache first.\")\n",
    "\n",
    "#     if sample_n > len(candidates):\n",
    "#         sample_n = len(candidates)\n",
    "\n",
    "#     sample = rng.sample(candidates, sample_n)\n",
    "\n",
    "#     # instantiate preprocessor once (same as cache builder)\n",
    "#     preproc = DICOMPreprocessorKaggle(target_shape=(CFG.in_chans, CFG.img_size, CFG.img_size))\n",
    "\n",
    "#     rows = []\n",
    "#     for sid, cache_path in tqdm(sample, desc=\"Checking cache vs raw\", unit=\"series\"):\n",
    "#         # load cached\n",
    "#         vol_cached = np.load(cache_path, mmap_mode=\"r\")\n",
    "#         if vol_cached.dtype != np.uint8:\n",
    "#             vol_cached = np.clip(vol_cached, 0, 255).astype(np.uint8)\n",
    "\n",
    "#         # recompute raw\n",
    "#         series_path = os.path.join(CFG.series_root, sid)\n",
    "#         vol_raw = preproc.process_series(series_path)\n",
    "#         if vol_raw.dtype != np.uint8:\n",
    "#             vol_raw = np.clip(vol_raw, 0, 255).astype(np.uint8)\n",
    "\n",
    "#         # compare\n",
    "#         try:\n",
    "#             metrics = _compare_pair(vol_cached, vol_raw)\n",
    "#         except AssertionError as e:\n",
    "#             metrics = {\"error\": str(e)}\n",
    "\n",
    "#         row = {\"SeriesInstanceUID\": sid, **metrics}\n",
    "#         rows.append(row)\n",
    "\n",
    "#     df = pd.DataFrame(rows)\n",
    "\n",
    "#     if verbose:\n",
    "#         if \"error\" in df.columns:\n",
    "#             has_error = df[\"error\"].notna() & (df[\"error\"].astype(str).str.len() > 0)\n",
    "#         else:\n",
    "#             has_error = pd.Series(False, index=df.index)\n",
    "\n",
    "#         ok = df[~has_error]\n",
    "#         if len(ok):\n",
    "#             psnr_vals = ok[\"psnr_u8\"].replace(np.inf, 100.0) if \"psnr_u8\" in ok else pd.Series(dtype=float)\n",
    "#             print(\n",
    "#                 \"Summary (no-error rows): \"\n",
    "#                 f\"mean mean_abs_u8={ok['mean_abs_u8'].mean():.4f}, \"\n",
    "#                 f\"mean max_abs_u8={ok['max_abs_u8'].mean():.2f}, \"\n",
    "#                 f\"mean eq_rate={ok['eq_rate'].mean():.4f}, \"\n",
    "#                 f\"mean psnr_u8={psnr_vals.mean():.2f} dB, \"\n",
    "#                 f\"mean mae_01={ok['mae_01'].mean():.6f}, \"\n",
    "#                 f\"mean rmse_01={ok['rmse_01'].mean():.6f}\"\n",
    "#             )\n",
    "#         bad = df[has_error]\n",
    "#         if len(bad):\n",
    "#             print(f\"{len(bad)} series had shape/dtype errors; inspect df for details.\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # ---- Run it (example) ----\n",
    "# df_check = verify_cache_vs_raw(sample_n=40, seed=42)\n",
    "# display(df_check.sort_values(\"mean_abs_u8\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import glob\n",
    "import ast\n",
    "\n",
    "def load_localizers_csv(csv_path: Optional[str], max_points_per_series: int = 3) -> Dict[str, List[dict]]:\n",
    "    \"\"\"\n",
    "    Load localizers with columns:\n",
    "      - SeriesInstanceUID\n",
    "      - SOPInstanceUID\n",
    "      - coordinates: string like \"{'x': 258.3, 'y': 261.4}\" or '{\"x\":..., \"y\":...}'\n",
    "      - location: optional text label\n",
    "\n",
    "    Returns: { sid: [ {'x': float|None, 'y': float|None, 'sop': str|None, 'loc': str|None}, ... ] }\n",
    "    \"\"\"\n",
    "    if not csv_path:\n",
    "        return {}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # normalize column names\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    sid_col = cols.get('seriesinstanceuid') or 'SeriesInstanceUID'\n",
    "    sop_col = cols.get('sopinstanceuid') or 'SOPInstanceUID'\n",
    "    coord_col = cols.get('coordinates') or 'coordinates'\n",
    "    loc_col = cols.get('location') or ('Location' if 'Location' in df.columns else None)\n",
    "\n",
    "    keep = [c for c in [sid_col, sop_col, coord_col, loc_col] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    by_sid: Dict[str, List[dict]] = defaultdict(list)\n",
    "    for _, r in df.iterrows():\n",
    "        sid = str(r[sid_col])\n",
    "        sop = str(r[sop_col]) if sop_col in r and pd.notna(r[sop_col]) else None\n",
    "        loc = str(r[loc_col]) if loc_col and pd.notna(r[loc_col]) else None\n",
    "\n",
    "        x = y = None\n",
    "        if coord_col in r and pd.notna(r[coord_col]):\n",
    "            s = str(r[coord_col]).strip()\n",
    "            try:\n",
    "                # handle both single-quote and JSON strings\n",
    "                if s.startswith('{') and s.endswith('}'):\n",
    "                    xy = ast.literal_eval(s)\n",
    "                    x = float(xy.get('x')) if xy.get('x') is not None else None\n",
    "                    y = float(xy.get('y')) if xy.get('y') is not None else None\n",
    "            except Exception:\n",
    "                x = y = None\n",
    "\n",
    "        by_sid[sid].append({'x': x, 'y': y, 'sop': sop, 'loc': loc})\n",
    "\n",
    "    # cap per series\n",
    "    for sid in list(by_sid.keys()):\n",
    "        by_sid[sid] = by_sid[sid][:max_points_per_series]\n",
    "\n",
    "    return dict(by_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_shard_roots() -> List[str]:\n",
    "    \"\"\"Find all cache shard folders under /kaggle/input/shards/*/cache_u8_{img}_shard*.\"\"\"\n",
    "    SHARDS_ROOT = \"D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/cache\"\n",
    "    pattern = os.path.join(SHARDS_ROOT, \"*\", f\"cache_u8_{CFG.img_size}_shard*\")\n",
    "    shard_roots = sorted([p for p in glob.glob(pattern) if os.path.isdir(p)])\n",
    "    if is_main_process():\n",
    "        print(\"Found shard roots:\", len(shard_roots))\n",
    "        for p in shard_roots[:8]:\n",
    "            print(\"  \", p)\n",
    "    return shard_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:34:05.501021Z",
     "iopub.status.busy": "2025-09-17T13:34:05.50082Z",
     "iopub.status.idle": "2025-09-17T13:34:05.523534Z",
     "shell.execute_reply": "2025-09-17T13:34:05.523026Z",
     "shell.execute_reply.started": "2025-09-17T13:34:05.501005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cached Dataset \n",
    "\n",
    "STRICT_CACHE_ONLY = True   # set False to allow on-the-fly fallback\n",
    "\n",
    "def _safe_crop_2d(img: np.ndarray, cx: int, cy: int, size: int) -> np.ndarray:\n",
    "    \"\"\"Center crop with clamping; returns (size,size).\"\"\"\n",
    "    H, W = img.shape\n",
    "    half = size // 2\n",
    "    x0 = max(0, cx - half); x1 = min(W, cx + half)\n",
    "    y0 = max(0, cy - half); y1 = min(H, cy + half)\n",
    "    crop = img[y0:y1, x0:x1]\n",
    "    # pad if we hit borders\n",
    "    if crop.shape[0] != size or crop.shape[1] != size:\n",
    "        pad_y = size - crop.shape[0]\n",
    "        pad_x = size - crop.shape[1]\n",
    "        crop = np.pad(crop,\n",
    "                      ((0, max(0,pad_y)), (0, max(0,pad_x))),\n",
    "                      mode='edge')\n",
    "        crop = crop[:size, :size]\n",
    "    return crop\n",
    "\n",
    "def _resize_2d(img: np.ndarray, out_hw: Tuple[int,int]) -> np.ndarray:\n",
    "    return cv2.resize(img, (out_hw[1], out_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def _map_localizer_to_cached_depth(\n",
    "    loc_z: Optional[float],\n",
    "    loc_f: Optional[int],\n",
    "    cached_depth: int,\n",
    "    orig_depth: Optional[int] = None\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Map a localizer z/f to the 0..cached_depth-1 index space.\n",
    "    - If we know orig_depth, linearly map: round( f / (orig_depth-1) * (cached_depth-1) ).\n",
    "    - Else if we have z as a [0..orig_depth) style value, same idea.\n",
    "    - Else fallback to middle slice.\n",
    "    \"\"\"\n",
    "    if orig_depth and loc_f is not None:\n",
    "        return int(np.clip(round(loc_f / max(1, (orig_depth-1)) * (cached_depth-1)), 0, cached_depth-1))\n",
    "    if orig_depth and loc_z is not None:\n",
    "        return int(np.clip(round(loc_z  / max(1, (orig_depth-1)) * (cached_depth-1)), 0, cached_depth-1))\n",
    "    # fallback middle slice\n",
    "    return cached_depth // 2\n",
    "\n",
    "import os, pydicom, numpy as np\n",
    "from functools import lru_cache\n",
    "\n",
    "def _rank_to_cached_idx(rank: int, orig_depth: int, cached_depth: int) -> int:\n",
    "    if orig_depth <= 1: \n",
    "        return cached_depth // 2\n",
    "    r = np.clip(rank, 0, orig_depth-1)\n",
    "    return int(round(r / (orig_depth - 1) * (cached_depth - 1)))\n",
    "\n",
    "@lru_cache(maxsize=512)\n",
    "def _build_sop_rank_map(series_dir: str) -> tuple[dict, int]:\n",
    "    \"\"\"\n",
    "    Returns (sop_to_rank, orig_depth) for a series, computed by sorting slices by z.\n",
    "    No pixel reads; fast.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    try:\n",
    "        for name in os.listdir(series_dir):\n",
    "            if not name.lower().endswith(\".dcm\"):\n",
    "                continue\n",
    "            path = os.path.join(series_dir, name)\n",
    "            ds = pydicom.dcmread(path, stop_before_pixels=True, force=True)\n",
    "            sop = str(getattr(ds, \"SOPInstanceUID\", os.path.splitext(name)[0]))\n",
    "            ipp = getattr(ds, \"ImagePositionPatient\", None)\n",
    "            z = float(ipp[2]) if ipp is not None and len(ipp) == 3 else float(getattr(ds, \"SliceLocation\", 0.0))\n",
    "            items.append((sop, z))\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not items:\n",
    "        return ({}, 0)\n",
    "    # sort by z, assign ranks 0..N-1\n",
    "    items.sort(key=lambda t: t[1])\n",
    "    sop_to_rank = {sop: i for i, (sop, _) in enumerate(items)}\n",
    "    return sop_to_rank, len(items)\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "def _sid_seed(sid: str, salt: str = \"locrand\") -> int:\n",
    "    h = hashlib.sha1((salt + sid).encode()).hexdigest()[:8]\n",
    "    return int(h, 16)\n",
    "\n",
    "def _random_local_points(H: int, W: int, K: int, rng: np.random.Generator) -> list[tuple[int,int]]:\n",
    "    if K <= 0: return []\n",
    "    xs = rng.integers(low=W//8, high=W - W//8, size=K)\n",
    "    ys = rng.integers(low=H//8, high=H - H//8, size=K)\n",
    "    return list(zip(xs.tolist(), ys.tolist()))\n",
    "\n",
    "class RSNADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        series_root: str,\n",
    "        preproc,  # DICOMPreprocessorKaggle\n",
    "        find_cached_path_fn,\n",
    "        localizers_csv_path: Optional[str] = None,\n",
    "        max_localizer_crops: int = 3,\n",
    "        local_crop_size: int = 128,\n",
    "        sid_to_orig_depth: Optional[Dict[str, int]] = None,  # if you have it\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.series_root = series_root\n",
    "        self.preproc = preproc\n",
    "        self.find_cached_path = find_cached_path_fn\n",
    "\n",
    "        # Localizers\n",
    "        localizers_csv_path = CFG.localizers_csv_path\n",
    "        self.localizers_map: Dict[str, List[dict]] = load_localizers_csv(\n",
    "            localizers_csv_path, max_points_per_series=max_localizer_crops\n",
    "        ) if localizers_csv_path else {}\n",
    "        self.max_localizer_crops = max_localizer_crops\n",
    "        self.local_crop_size = int(local_crop_size)\n",
    "        self.sid_to_orig_depth = sid_to_orig_depth or {}\n",
    "        self._epoch = 0\n",
    "        self._rng = np.random.default_rng(CFG.seed)\n",
    "\n",
    "    def set_epoch(self, e: int):\n",
    "        self._epoch = int(e)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sid = str(row[\"SeriesInstanceUID\"])\n",
    "        cp = self.find_cached_path(sid, CFG.img_size)\n",
    "\n",
    "        # tiny helper: 2D resize to (H,W) \n",
    "        import cv2\n",
    "        def resize2d(arr2d, out_hw):\n",
    "            h, w = int(out_hw[0]), int(out_hw[1])\n",
    "            if arr2d.shape != (h, w):\n",
    "                return cv2.resize(arr2d, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "            return arr2d\n",
    "\n",
    "        # load cached (or preprocess) \n",
    "        if cp is not None:\n",
    "            vol_u8 = np.load(cp, mmap_mode=\"r\")  # (C,H,W) usually C==32\n",
    "        else:\n",
    "            if STRICT_CACHE_ONLY:\n",
    "                raise FileNotFoundError(f\"Not found in cache: {sid}_{CFG.img_size}.npy\")\n",
    "            series_path = os.path.join(self.series_root, sid)\n",
    "            vol = self.preproc.process_series(series_path)  # (32,H,W) float32 0..255\n",
    "            vol_u8 = vol if vol.dtype == np.uint8 else np.clip(vol, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # sanity / resize base stack to (H,W) \n",
    "        if vol_u8.ndim != 3:\n",
    "            raise ValueError(f\"Expected (C,H,W), got {tuple(vol_u8.shape)}\")\n",
    "        base_C = vol_u8.shape[0]\n",
    "        if base_C < 32:\n",
    "            raise ValueError(f\"Expected at least 32 slices, got {base_C}\")\n",
    "\n",
    "        H, W = vol_u8.shape[1], vol_u8.shape[2]\n",
    "        if (H != CFG.img_size) or (W != CFG.img_size):\n",
    "            vol_u8 = np.stack(\n",
    "                [cv2.resize(vol_u8[c], (CFG.img_size, CFG.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "                for c in range(vol_u8.shape[0])],\n",
    "                axis=0\n",
    "            )\n",
    "            H, W = CFG.img_size, CFG.img_size  # recompute\n",
    "\n",
    "        #  optional vesselness sidecar (added b4 localizers) \n",
    "        # sidecar filename: \"<sid>_<img>_vessel_u8.npy\"\n",
    "        if cp is not None:\n",
    "            guess1 = cp.replace(f\"_{CFG.img_size}.npy\", f\"_{CFG.img_size}_vessel_u8.npy\")\n",
    "            guess2 = os.path.join(os.path.dirname(cp), f\"{sid}_{CFG.img_size}_vessel_u8.npy\")\n",
    "            sidecar_path = guess1 if os.path.exists(guess1) else (guess2 if os.path.exists(guess2) else None)\n",
    "        else:\n",
    "            sidecar_path = None\n",
    "\n",
    "        if getattr(CFG, \"use_vessel_sidecar\", True) and sidecar_path is not None:\n",
    "            vess = np.load(sidecar_path, mmap_mode=\"r\")  # (H,W) or (32,H,W)\n",
    "            mode = getattr(CFG, \"vessel_sidecar_mode\", \"mip\")  # \"mip\" or \"per_slice\"\n",
    "\n",
    "            if vess.ndim == 2:\n",
    "                vess2d = resize2d(np.asarray(vess), (H, W)).astype(np.uint8)\n",
    "                vol_u8 = np.concatenate([vol_u8, vess2d[np.newaxis, ...]], axis=0)\n",
    "\n",
    "            elif vess.ndim == 3:\n",
    "                if vess.shape[-2:] != (H, W):\n",
    "                    vess = np.stack([resize2d(vess[z], (H, W)) for z in range(vess.shape[0])], axis=0)\n",
    "                if mode == \"per_slice\":\n",
    "                    vol_u8 = np.concatenate([vol_u8, vess.astype(np.uint8)], axis=0)        # +32\n",
    "                else:\n",
    "                    vess2d = np.asarray(vess).max(axis=0).astype(np.uint8)                  # +1\n",
    "                    vol_u8 = np.concatenate([vol_u8, vess2d[np.newaxis, ...]], axis=0)\n",
    "\n",
    "        # localizer-based extra channels (fixed K) \n",
    "        local_chans = []\n",
    "        K = self.max_localizer_crops\n",
    "        force_random = False\n",
    "\n",
    "        pgo = getattr(CFG, \"p_global_localizer_off\", 0.0)\n",
    "        if pgo > 0:\n",
    "            grng = np.random.default_rng(hash(('global_off', sid, self._epoch, CFG.seed)) & 0xffffffff)\n",
    "            if grng.random() < pgo:\n",
    "                force_random = True\n",
    "\n",
    "        if CFG.use_localizers and K > 0:\n",
    "            # Build MIPs from the first 32 slices only (do not include sidecar channels)\n",
    "            vol_for_mip = vol_u8[:32] if vol_u8.shape[0] >= 32 else vol_u8\n",
    "            cached_depth = vol_for_mip.shape[0]\n",
    "\n",
    "            hint_orig_depth = self.sid_to_orig_depth.get(sid, None)\n",
    "            series_dir = os.path.join(self.series_root, sid)\n",
    "            if os.path.isdir(series_dir):\n",
    "                sop_to_rank, hdr_orig_depth = _build_sop_rank_map(series_dir)\n",
    "            else:\n",
    "                sop_to_rank, hdr_orig_depth = ({}, 0)\n",
    "            use_orig_depth = hint_orig_depth or hdr_orig_depth or cached_depth\n",
    "\n",
    "            rng = np.random.default_rng(hash((sid, self._epoch, CFG.seed)) & 0xffffffff)\n",
    "            use_random = force_random or (rng.random() < CFG.p_localizer_dropout)\n",
    "\n",
    "            pts = self.localizers_map.get(sid, [])\n",
    "            use_pts = (len(pts) > 0) and (not use_random)\n",
    "\n",
    "            if use_pts:\n",
    "                for p in pts[:K]:\n",
    "                    sop = p.get('sop')\n",
    "                    if sop and sop in sop_to_rank and use_orig_depth > 0:\n",
    "                        rank = sop_to_rank[sop]\n",
    "                        z_idx = _rank_to_cached_idx(rank, use_orig_depth, cached_depth)\n",
    "                    else:\n",
    "                        z_idx = _map_localizer_to_cached_depth(\n",
    "                            p.get('z'), p.get('f'),\n",
    "                            cached_depth=cached_depth, orig_depth=use_orig_depth\n",
    "                        )\n",
    "                    z0 = max(0, z_idx - 8); z1 = min(cached_depth, z_idx + 9)\n",
    "                    slab = vol_for_mip[z0:z1]\n",
    "                    mip = slab.max(axis=0) if slab.size else np.zeros((H, W), dtype=vol_u8.dtype)\n",
    "\n",
    "                    px, py = p.get('x'), p.get('y')\n",
    "                    if px is None or py is None:\n",
    "                        cx, cy = W // 2, H // 2\n",
    "                    else:\n",
    "                        cx = int(round(np.clip(px, 0, W - 1)))\n",
    "                        cy = int(round(np.clip(py, 0, H - 1)))\n",
    "\n",
    "                    crop = _safe_crop_2d(mip, cx, cy, size=self.local_crop_size)\n",
    "                    crop_full = resize2d(crop, (H, W))\n",
    "                    local_chans.append(crop_full[np.newaxis, ...])\n",
    "\n",
    "            if len(local_chans) < K:\n",
    "                need = K - len(local_chans)\n",
    "                z_rng = np.random.default_rng(hash((sid, 'z', self._epoch, CFG.seed)) & 0xffffffff)\n",
    "                cz = z_rng.integers(low=0, high=max(1, cached_depth), size=need)\n",
    "                for i in range(need):\n",
    "                    z_idx = int(cz[i])\n",
    "                    z0 = max(0, z_idx - 8); z1 = min(cached_depth, z_idx + 9)\n",
    "                    slab = vol_for_mip[z0:z1]\n",
    "                    mip = slab.max(axis=0) if slab.size else np.zeros((H, W), dtype=vol_u8.dtype)\n",
    "\n",
    "                    rrng = np.random.default_rng(hash((sid, 'xy', i, self._epoch, CFG.seed)) & 0xffffffff)\n",
    "                    (cx, cy) = _random_local_points(H, W, 1, rrng)[0]\n",
    "\n",
    "                    crop = _safe_crop_2d(mip, cx, cy, size=self.local_crop_size)\n",
    "                    crop_full = resize2d(crop, (H, W))\n",
    "                    local_chans.append(crop_full[np.newaxis, ...])\n",
    "\n",
    "            if len(local_chans) > K:\n",
    "                local_chans = local_chans[:K]\n",
    "            extra = np.concatenate(local_chans, axis=0) if local_chans else np.zeros((K, H, W), dtype=vol_u8.dtype)\n",
    "            vol_u8 = np.concatenate([vol_u8, extra], axis=0)\n",
    "\n",
    "        #  final channel alignment to CFG.in_chans \n",
    "        C = vol_u8.shape[0]\n",
    "        target_C = int(CFG.in_chans)\n",
    "        if C < target_C:\n",
    "            pad = np.zeros((target_C - C, H, W), dtype=vol_u8.dtype)\n",
    "            vol_u8 = np.concatenate([vol_u8, pad], axis=0)\n",
    "        elif C > target_C:\n",
    "            vol_u8 = vol_u8[:target_C]\n",
    "\n",
    "        # --- to tensor ---\n",
    "        x = torch.from_numpy(np.asarray(vol_u8)).to(torch.float32).div_(255.0)  # (C,H,W) in [0,1]\n",
    "        y = torch.tensor(row[LABEL_COLS].values.astype(np.float32))\n",
    "        return x, y, sid\n",
    "\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.df.iloc[idx]\n",
    "    #     sid = str(row[\"SeriesInstanceUID\"])\n",
    "    #     cp = self.find_cached_path(sid, CFG.img_size)\n",
    "    \n",
    "    #     # --- load cached (or preprocess) ---\n",
    "    #     if cp is not None:\n",
    "    #         vol_u8 = np.load(cp, mmap_mode=\"r\")  # (C,H,W) or (32,H,W)\n",
    "    #     else:\n",
    "    #         if STRICT_CACHE_ONLY:\n",
    "    #             raise FileNotFoundError(f\"Not found in cache: {sid}_{CFG.img_size}.npy\")\n",
    "    #         series_path = os.path.join(self.series_root, sid)\n",
    "    #         vol = self.preproc.process_series(series_path)  # (32,H,W) float32 0..255\n",
    "    #         vol_u8 = vol if vol.dtype == np.uint8 else np.clip(vol, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    #     # --- sanity checks / shapes ---\n",
    "    #     if vol_u8.ndim != 3:\n",
    "    #         raise ValueError(f\"Expected (C,H,W), got {tuple(vol_u8.shape)}\")\n",
    "    #     base_C = vol_u8.shape[0]\n",
    "    #     if base_C < 32:\n",
    "    #         raise ValueError(f\"Expected at least 32 slices, got {base_C}\")\n",
    "    \n",
    "    #     H, W = vol_u8.shape[1], vol_u8.shape[2]\n",
    "    #     if (H != CFG.img_size) or (W != CFG.img_size):\n",
    "    #         import cv2\n",
    "    #         vol_u8 = np.stack(\n",
    "    #             [cv2.resize(vol_u8[c], (CFG.img_size, CFG.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "    #              for c in range(vol_u8.shape[0])],\n",
    "    #             axis=0\n",
    "    #         )\n",
    "    #     # recompute after possible resize\n",
    "    #     H, W = vol_u8.shape[1], vol_u8.shape[2]\n",
    "\n",
    "    #     # ---- localizer-based extra channels (fixed K) ----\n",
    "    #     local_chans = []\n",
    "    #     K = self.max_localizer_crops\n",
    "    #     force_random = False\n",
    "\n",
    "    #     # Global-off: keep K the same, but ignore real points for this sample\n",
    "    #     pgo = getattr(CFG, \"p_global_localizer_off\", 0.0)\n",
    "    #     if pgo > 0:\n",
    "    #         grng = np.random.default_rng(hash(('global_off', sid, self._epoch, CFG.seed)) & 0xffffffff)\n",
    "    #         if grng.random() < pgo:\n",
    "    #             force_random = True\n",
    "\n",
    "    #     if CFG.use_localizers and K > 0:\n",
    "    #         vol_for_mip = vol_u8[:32] if base_C >= 32 else vol_u8\n",
    "    #         cached_depth = vol_for_mip.shape[0]\n",
    "\n",
    "    #         hint_orig_depth = self.sid_to_orig_depth.get(sid, None)\n",
    "    #         series_dir = os.path.join(self.series_root, sid)\n",
    "    #         if os.path.isdir(series_dir):\n",
    "    #             sop_to_rank, hdr_orig_depth = _build_sop_rank_map(series_dir)\n",
    "    #         else:\n",
    "    #             sop_to_rank, hdr_orig_depth = ({}, 0)\n",
    "    #         use_orig_depth = hint_orig_depth or hdr_orig_depth or cached_depth\n",
    "\n",
    "    #         rng = np.random.default_rng(hash((sid, self._epoch, CFG.seed)) & 0xffffffff)\n",
    "    #         # anti-leakage dropout OR forced-random from global-off\n",
    "    #         use_random = force_random or (rng.random() < CFG.p_localizer_dropout)\n",
    "\n",
    "    #         pts = self.localizers_map.get(sid, [])\n",
    "    #         use_pts = (len(pts) > 0) and (not use_random)\n",
    "\n",
    "    #         if use_pts:\n",
    "    #             for p in pts[:K]:\n",
    "    #                 sop = p.get('sop')\n",
    "    #                 if sop and sop in sop_to_rank and use_orig_depth > 0:\n",
    "    #                     rank = sop_to_rank[sop]\n",
    "    #                     z_idx = _rank_to_cached_idx(rank, use_orig_depth, cached_depth)\n",
    "    #                 else:\n",
    "    #                     z_idx = _map_localizer_to_cached_depth(\n",
    "    #                         p.get('z'), p.get('f'), cached_depth=cached_depth, orig_depth=use_orig_depth\n",
    "    #                     )\n",
    "    #                 z0 = max(0, z_idx - 8); z1 = min(cached_depth, z_idx + 9)\n",
    "    #                 slab = vol_for_mip[z0:z1]\n",
    "    #                 mip = slab.max(axis=0) if slab.size else np.zeros((H, W), dtype=vol_u8.dtype)\n",
    "\n",
    "    #                 px, py = p.get('x'), p.get('y')\n",
    "    #                 if px is None or py is None:\n",
    "    #                     cx, cy = W // 2, H // 2\n",
    "    #                 else:\n",
    "    #                     cx = int(round(np.clip(px, 0, W - 1)))\n",
    "    #                     cy = int(round(np.clip(py, 0, H - 1)))\n",
    "\n",
    "    #                 crop = _safe_crop_2d(mip, cx, cy, size=self.local_crop_size)\n",
    "    #                 crop_full = _resize_2d(crop, (H, W))\n",
    "    #                 local_chans.append(crop_full[np.newaxis, ...])\n",
    "\n",
    "    #         # Fill up to K with deterministic random crops (epoch-varying)\n",
    "    #         if len(local_chans) < K:\n",
    "    #             need = K - len(local_chans)\n",
    "    #             z_rng = np.random.default_rng(hash((sid, 'z', self._epoch, CFG.seed)) & 0xffffffff)\n",
    "    #             cz = z_rng.integers(low=0, high=max(1, cached_depth), size=need)\n",
    "    #             for i in range(need):\n",
    "    #                 z_idx = int(cz[i])\n",
    "    #                 z0 = max(0, z_idx - 8); z1 = min(cached_depth, z_idx + 9)\n",
    "    #                 slab = vol_for_mip[z0:z1]\n",
    "    #                 mip = slab.max(axis=0) if slab.size else np.zeros((H, W), dtype=vol_u8.dtype)\n",
    "\n",
    "    #                 rrng = np.random.default_rng(hash((sid, 'xy', i, self._epoch, CFG.seed)) & 0xffffffff)\n",
    "    #                 (cx, cy) = _random_local_points(H, W, 1, rrng)[0]\n",
    "\n",
    "    #                 crop = _safe_crop_2d(mip, cx, cy, size=self.local_crop_size)\n",
    "    #                 crop_full = _resize_2d(crop, (H, W))\n",
    "    #                 local_chans.append(crop_full[np.newaxis, ...])\n",
    "\n",
    "    #         # Trim (paranoia) and concat\n",
    "    #         if len(local_chans) > K:\n",
    "    #             local_chans = local_chans[:K]\n",
    "    #         extra = np.concatenate(local_chans, axis=0) if local_chans else np.zeros((K, H, W), dtype=vol_u8.dtype)\n",
    "    #         vol_u8 = np.concatenate([vol_u8, extra], axis=0)\n",
    "\n",
    "\n",
    "    #     # --- to tensor ---\n",
    "    #     x = torch.from_numpy(np.asarray(vol_u8)).to(torch.float32).div_(255.0)  # (C,H,W)\n",
    "    #     y = torch.tensor(row[LABEL_COLS].values.astype(np.float32))\n",
    "    #     return x, y, sid\n",
    "\n",
    "\n",
    "#  smarter stratification helpers \n",
    "\n",
    "def _age_to_bin(s: pd.Series) -> pd.Series:\n",
    "    # PatientAge can be like \"067Y\" or numeric; coerce to number of years\n",
    "    raw = pd.to_numeric(s.astype(str).str.extract(r'(\\d+)')[0], errors='coerce')\n",
    "    # decade-ish bins; fill missing as -1\n",
    "    bins = pd.cut(raw, bins=[0,30,40,50,60,70,80,200], labels=False, include_lowest=True)\n",
    "    return bins.fillna(-1).astype(int)\n",
    "\n",
    "def _slice_bin_for_series(series_root: str, sid: str) -> int:\n",
    "    \"\"\"Super-cheap proxy for series 'size': count DICOM files in folder.\"\"\"\n",
    "    p = os.path.join(series_root, str(sid))\n",
    "    n = 0\n",
    "    try:\n",
    "        with os.scandir(p) as it:\n",
    "            for e in it:\n",
    "                if e.is_file():\n",
    "                    n += 1\n",
    "                    # (optional) early stop to keep it fast\n",
    "                    if n > 300: \n",
    "                        break\n",
    "    except FileNotFoundError:\n",
    "        n = 0\n",
    "    # bucketize\n",
    "    if n <= 64:   return 0\n",
    "    if n <= 128:  return 1\n",
    "    if n <= 256:  return 2\n",
    "    return 3\n",
    "\n",
    "def _make_strat_key(df: pd.DataFrame) -> pd.Series:\n",
    "    # core target\n",
    "    ap = df[\"Aneurysm Present\"].astype(int)\n",
    "\n",
    "    # simple categorical covariates (normalized)\n",
    "    mod = df.get(\"Modality\", \"UNK\").astype(str).str.upper().fillna(\"UNK\")\n",
    "    sex = df.get(\"PatientSex\", \"UNK\").astype(str).str.upper().fillna(\"UNK\")\n",
    "\n",
    "    # age bins\n",
    "    ageb = _age_to_bin(df.get(\"PatientAge\", pd.Series([-1]*len(df))))\n",
    "\n",
    "    # slice-count bins (very fast directory count)\n",
    "    # note: uses CFG.series_root and SeriesInstanceUID\n",
    "    slb = df[\"SeriesInstanceUID\"].astype(str).apply(lambda sid: _slice_bin_for_series(CFG.series_root, sid))\n",
    "\n",
    "    # compose a single strat label\n",
    "    key = (\n",
    "        ap.astype(str) + \"_\" +\n",
    "        mod + \"_\" +\n",
    "        sex + \"_\" +\n",
    "        ageb.astype(str) + \"_\" +\n",
    "        pd.Series(slb, index=df.index).astype(str)\n",
    "    )\n",
    "    return key\n",
    "\n",
    "def build_folds() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load CSV, keep existing series, add fold column with composite stratification.\"\"\"\n",
    "    df = pd.read_csv(CFG.train_csv)\n",
    "\n",
    "    # keep only series that actually exist on disk\n",
    "    exists = df[\"SeriesInstanceUID\"].apply(lambda u: os.path.isdir(os.path.join(CFG.series_root, str(u))))\n",
    "    df = df[exists].reset_index(drop=True)\n",
    "\n",
    "    # composite strat key: target + modality + sex + age bin + slice-count bin\n",
    "    strat_key = _make_strat_key(df)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
    "    df[\"fold\"] = -1\n",
    "    for fold_i, (_, val_idx) in enumerate(skf.split(df, strat_key)):\n",
    "        df.loc[val_idx, \"fold\"] = fold_i\n",
    "\n",
    "    fold_idx = 0  # choose default here; you can pass a different one into build_loaders\n",
    "    train_df = df[df[\"fold\"] != fold_idx].reset_index(drop=True)\n",
    "    val_df   = df[df[\"fold\"] == fold_idx].reset_index(drop=True)\n",
    "    return df, train_df, val_df\n",
    "\n",
    "def load_sid_to_orig_depth(\n",
    "    shard_roots: list[str],\n",
    "    img_size: int,\n",
    "    extra_search_roots: list[str] | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Recursively search under each root for 'manifest_{img_size}.parquet' and build:\n",
    "        { SeriesInstanceUID -> orig_depth }\n",
    "    Works for paths like:\n",
    "      D:/.../cache/SHARD_ID=0/cache_u8_384_shard00/manifest_384.parquet\n",
    "    \"\"\"\n",
    "    # Collect candidate manifest files (recursive, Windows-safe)\n",
    "    roots = (shard_roots or []) + (extra_search_roots or [])\n",
    "    man_paths = set()\n",
    "    for root in roots:\n",
    "        if not root or not os.path.isdir(root):\n",
    "            continue\n",
    "        # same-dir check\n",
    "        p = os.path.join(root, f\"manifest_{img_size}.parquet\")\n",
    "        if os.path.exists(p):\n",
    "            man_paths.add(os.path.normpath(p))\n",
    "        # recursive search (any depth)\n",
    "        pattern = os.path.join(root, \"**\", f\"manifest_{img_size}.parquet\")\n",
    "        for mp in glob.glob(pattern, recursive=True):\n",
    "            man_paths.add(os.path.normpath(mp))\n",
    "\n",
    "    if not man_paths:\n",
    "        if verbose:\n",
    "            print(\"[manifest] No manifest files found under provided roots.\")\n",
    "        return {}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[manifest] Found {len(man_paths)} manifest(s).\")\n",
    "\n",
    "    sid_to_depth: dict[str, int] = {}\n",
    "    for mp in sorted(man_paths):\n",
    "        try:\n",
    "            m = pd.read_parquet(mp)                         \n",
    "        except Exception:\n",
    "            m = pd.read_parquet(mp, engine=\"fastparquet\") # fallback\n",
    "        if not {\"SeriesInstanceUID\", \"orig_depth\"}.issubset(m.columns):\n",
    "            continue\n",
    "        sub = m[[\"SeriesInstanceUID\", \"orig_depth\"]].dropna(subset=[\"orig_depth\"])\n",
    "        for sid, od in zip(sub[\"SeriesInstanceUID\"].astype(str), sub[\"orig_depth\"].astype(int)):\n",
    "            # \"latest wins\" if duplicates across shards\n",
    "            sid_to_depth[sid] = int(od)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[manifest] Loaded orig_depth for {len(sid_to_depth)} series.\")\n",
    "    return sid_to_depth\n",
    "\n",
    "\n",
    "def build_loaders(fold_idx: int = 0):\n",
    "    \"\"\"\n",
    "    Build train/val DataLoaders with DistributedSampler if WORLD_SIZE>1.\n",
    "    Uses the same composite stratification as build_folds().\n",
    "    \"\"\"\n",
    "    # discover cache + resolver\n",
    "    shard_roots = discover_shard_roots()\n",
    "\n",
    "    # build an O(1) index: (uid, size) -> path  [RECURSIVE!]\n",
    "    uid_to_path = {}\n",
    "    pattern = f\"*_{CFG.img_size}.npy\"\n",
    "    for root in shard_roots:\n",
    "        for p in glob.glob(os.path.join(root, \"**\", pattern), recursive=True):\n",
    "            fname = os.path.basename(p)            # e.g. \"1.2.840..._384.npy\"\n",
    "            uid, size_part = fname.rsplit(\"_\", 1)  # [\"1.2.840...\", \"384.npy\"]\n",
    "            size = int(size_part.split(\".\")[0])    # 384\n",
    "            uid_to_path[(uid, size)] = p\n",
    "\n",
    "    def find_cached_path(uid: str, img_size: int) -> str | None:\n",
    "        return uid_to_path.get((uid, img_size))\n",
    "    \n",
    "    find_cached_path_fn = find_cached_path\n",
    "    # build SeriesInstanceUID -> orig_depth from per-shard manifests\n",
    "    sid_to_orig_depth = load_sid_to_orig_depth(shard_roots, CFG.img_size)\n",
    "    \n",
    "    # distributed env\n",
    "    local_rank, rank, world_size, is_distributed = setup_distributed()\n",
    "    seed_everything(CFG.seed)\n",
    "\n",
    "    # folds (recompute the same split deterministically)\n",
    "    df = pd.read_csv(CFG.train_csv)\n",
    "    exists = df[\"SeriesInstanceUID\"].apply(lambda u: os.path.isdir(os.path.join(CFG.series_root, str(u))))\n",
    "    df = df[exists].reset_index(drop=True)\n",
    "\n",
    "    strat_key = _make_strat_key(df)\n",
    "    skf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
    "    df[\"fold\"] = -1\n",
    "    for fi, (_, val_idx) in enumerate(skf.split(df, strat_key)):\n",
    "        df.loc[val_idx, \"fold\"] = fi\n",
    "\n",
    "    train_df = df[df[\"fold\"] != fold_idx].reset_index(drop=True)\n",
    "    val_df   = df[df[\"fold\"] == fold_idx].reset_index(drop=True)\n",
    "\n",
    "    # optional: quick cache check (main process only)\n",
    "    if is_main_process() and len(shard_roots) > 0 and len(df) > 0:\n",
    "        sample_sid = str(df.iloc[0][\"SeriesInstanceUID\"])\n",
    "        print(\"Sample cached path:\", find_cached_path_fn(sample_sid, CFG.img_size))\n",
    "\n",
    "    # datasets\n",
    "    preproc = DICOMPreprocessorKaggle(target_shape=(CFG.base_slices, CFG.img_size, CFG.img_size))\n",
    "    \n",
    "    train_ds = RSNADataset(\n",
    "        train_df, CFG.series_root, preproc, find_cached_path_fn, \n",
    "        localizers_csv_path=getattr(CFG, \"localizers_csv_path\", None),\n",
    "        max_localizer_crops=getattr(CFG, \"max_localizer_crops\", 3),\n",
    "        local_crop_size=getattr(CFG, \"local_crop_size\", 128),\n",
    "        sid_to_orig_depth=sid_to_orig_depth,)\n",
    "    \n",
    "    val_ds = RSNADataset(\n",
    "        val_df,   CFG.series_root, preproc, find_cached_path_fn,\n",
    "        localizers_csv_path=getattr(CFG, \"localizers_csv_path\", None),\n",
    "        max_localizer_crops=getattr(CFG, \"max_localizer_crops\", 3),\n",
    "        local_crop_size=getattr(CFG, \"local_crop_size\", 128),\n",
    "        sid_to_orig_depth=sid_to_orig_depth,\n",
    "    )\n",
    "\n",
    "    # samplers\n",
    "    if world_size > 1:\n",
    "        train_sampler = DistributedSampler(train_ds, num_replicas=world_size, rank=rank, shuffle=True, drop_last=True)\n",
    "        val_sampler   = DistributedSampler(val_ds,   num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        val_sampler   = None\n",
    "\n",
    "    # loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG.batch_size,             # per-GPU\n",
    "        sampler=train_sampler,\n",
    "        shuffle=(train_sampler is None),\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        persistent_workers=(CFG.num_workers > 0),\n",
    "        prefetch_factor=2 if CFG.num_workers > 0 else None,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=max(1, CFG.batch_size // 2),\n",
    "        sampler=val_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(CFG.num_workers > 0),\n",
    "        prefetch_factor=2 if CFG.num_workers > 0 else None,\n",
    "    )\n",
    "\n",
    "    if is_main_process():\n",
    "        print(f\"World size: {world_size}  |  Rank: {rank}  |  Local rank: {local_rank}\")\n",
    "        print(f\"Train: {len(train_ds)} | Val: {len(val_ds)}\")\n",
    "\n",
    "    return train_loader, val_loader, fold_idx, world_size, rank, local_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:34:05.525825Z",
     "iopub.status.busy": "2025-09-17T13:34:05.5256Z",
     "iopub.status.idle": "2025-09-17T13:34:05.552153Z",
     "shell.execute_reply": "2025-09-17T13:34:05.551288Z",
     "shell.execute_reply.started": "2025-09-17T13:34:05.525808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BCEWithLogitsSmooth(nn.Module):\n",
    "    def __init__(self, smoothing=0.0, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.pos_weight = pos_weight\n",
    "    def forward(self, logits, targets):\n",
    "        if self.smoothing > 0.0:\n",
    "            targets = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "        return nn.functional.binary_cross_entropy_with_logits(logits, targets, pos_weight=self.pos_weight)\n",
    "\n",
    "class FocalWithLogits(nn.Module):\n",
    "    def __init__(self, gamma=1.5, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma; self.pos_weight = pos_weight\n",
    "    def forward(self, logits, targets):\n",
    "        bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, pos_weight=self.pos_weight, reduction=\"none\")\n",
    "        p = torch.sigmoid(logits); pt = p*targets + (1-p)*(1-targets)\n",
    "        return ((1-pt)**self.gamma * bce).mean()\n",
    "\n",
    "def cosine_sched(step, total_steps, base_lr, min_lr, warmup_steps):\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * (step / max(1, warmup_steps))\n",
    "    t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    return min_lr + 0.5*(base_lr - min_lr)*(1 + math.cos(math.pi*t))\n",
    "\n",
    "def make_model():\n",
    "    model = timm.create_model(\n",
    "        CFG.model_name,\n",
    "        in_chans=CFG.in_chans,\n",
    "        num_classes=CFG.num_classes,\n",
    "        img_size=CFG.img_size,\n",
    "        pretrained=True\n",
    "    )\n",
    "    if USE_CHANNELS_LAST:\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "    # optional compile for a few extra %\n",
    "    if USE_TORCH_COMPILE:\n",
    "        try:\n",
    "            model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return model\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, train_loader, val_loader, fold: int):\n",
    "        # DDP env\n",
    "        self.local_rank, self.rank, self.world, self.is_distributed = setup_distributed()\n",
    "        dtype = torch.float16 if AMP_DTYPE == \"bf16\" else torch.bfloat16 if AMP_DTYPE == \"bf16\" else None\n",
    "\n",
    "        # device & model\n",
    "        self.device = torch.device(\"cuda\", self.local_rank) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        torch.cuda.set_device(self.local_rank if torch.cuda.is_available() else 0)\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "        self.model = make_model().to(self.device)\n",
    "\n",
    "        if self.is_distributed:\n",
    "            # important: broadcast buffers True, find_unused False for speed\n",
    "            self.model = DDP(\n",
    "                self.model,\n",
    "                device_ids=[self.local_rank],\n",
    "                output_device=self.local_rank,\n",
    "                broadcast_buffers=True,\n",
    "                find_unused_parameters=False,\n",
    "            )\n",
    "\n",
    "        # loss / opt / scaler\n",
    "        pos_weight = torch.tensor([CFG.pos_weight]*CFG.num_classes, device=self.device)\n",
    "        self.criterion = (FocalWithLogits(CFG.focal_gamma, pos_weight)\n",
    "                          if CFG.focal_loss else BCEWithLogitsSmooth(CFG.label_smoothing, pos_weight))\n",
    "\n",
    "        base_lr = CFG.lr  # no scaling; keeps LR stable moving from 1->2 GPUs\n",
    "        # base_lr = scale_lr_for_world_size(CFG.lr)  # linear scale with world size\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=base_lr, weight_decay=CFG.weight_decay)\n",
    "        self.global_step = 0\n",
    "        self.ema_decay = 0.9998\n",
    "        base_model = self.model.module if isinstance(self.model, DDP) else self.model\n",
    "        self.ema = copy.deepcopy(base_model).eval()\n",
    "        # ensure EMA is on the same device & memory format\n",
    "        self.ema.to(self.device)\n",
    "        if USE_CHANNELS_LAST:\n",
    "            self.ema.to(memory_format=torch.channels_last)\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "            \n",
    "        self.base_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "        self.scaler = GradScaler(enabled=CFG.use_amp and AMP_DTYPE == \"bf16\")  # GradScaler is for fp16 only\n",
    "\n",
    "        self.train_loader, self.val_loader = train_loader, val_loader\n",
    "        self.fold = fold\n",
    "        self.fold_dir = os.path.join(CFG.out_dir, f\"{CFG.save_name}_seed{CFG.seed}_fold{self.fold}\")\n",
    "        os.makedirs(self.fold_dir, exist_ok=True)\n",
    "        self.total_steps = CFG.epochs * len(train_loader)\n",
    "        self.warmup_steps = int(CFG.warmup_epochs * len(train_loader))\n",
    "        os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "        self.best_auc = -1.0\n",
    "\n",
    "        # remember autocast dtype\n",
    "        self.autocast_dtype = torch.float16 if AMP_DTYPE == \"bf16\" else (torch.bfloat16 if AMP_DTYPE == \"bf16\" else None)\n",
    "\n",
    "    def _cast_input(self, x):\n",
    "        # Keep channels_last for better memory access on T4\n",
    "        if USE_CHANNELS_LAST and x.ndim == 4:\n",
    "            x = x.contiguous(memory_format=torch.channels_last)\n",
    "        return x\n",
    "    \n",
    "    def _update_ema(self, step: int):\n",
    "        \"\"\"EMA update: params with decay; buffers (BN stats) copied 1:1 each step.\n",
    "           On very first step, copy whole state 1:1 (warm start).\"\"\"\n",
    "        src = self.model.module if isinstance(self.model, DDP) else self.model\n",
    "    \n",
    "        # Warm-start EMA at first step to avoid bias\n",
    "        if step == 0:\n",
    "            self.ema.load_state_dict(src.state_dict(), strict=True)\n",
    "            return\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            # 1) EMA for parameters\n",
    "            for pe, pm in zip(self.ema.parameters(), src.parameters()):\n",
    "                pe.mul_(self.ema_decay).add_(pm.detach(), alpha=1.0 - self.ema_decay)\n",
    "            # 2) Direct copy for buffers (BN running stats, etc.)\n",
    "            for be, bm in zip(self.ema.buffers(), src.buffers()):\n",
    "                be.copy_(bm.detach())\n",
    "    \n",
    "    def one_epoch(self, epoch):\n",
    "        if self.is_distributed and hasattr(self.train_loader, \"sampler\") and hasattr(self.train_loader.sampler, \"set_epoch\"):\n",
    "            self.train_loader.sampler.set_epoch(epoch)\n",
    "         # NEW: always update dataset epoch (train & val)\n",
    "        if hasattr(self.train_loader, \"dataset\") and hasattr(self.train_loader.dataset, \"set_epoch\"):\n",
    "            self.train_loader.dataset.set_epoch(epoch)\n",
    "        if hasattr(self.val_loader, \"dataset\") and hasattr(self.val_loader.dataset, \"set_epoch\"):\n",
    "            self.val_loader.dataset.set_epoch(0)  # keep val deterministic\n",
    "    \n",
    "        self.model.train()\n",
    "        running = 0.0\n",
    "        start_step = epoch * len(self.train_loader)\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        iterator = self.train_loader\n",
    "        if is_main_process():\n",
    "            iterator = tqdm(self.train_loader, total=len(self.train_loader), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "\n",
    "        t0 = time()\n",
    "        for it, (x,y, _) in enumerate(iterator):\n",
    "            x = self._cast_input(x.to(self.device, non_blocking=True))\n",
    "            y = y.to(self.device, non_blocking=True)\n",
    "\n",
    "            lr = cosine_sched(start_step+it, self.total_steps, self.base_lr, CFG.min_lr, self.warmup_steps)\n",
    "            for pg in self.optimizer.param_groups: pg[\"lr\"] = lr\n",
    "\n",
    "            # forward \n",
    "            if CFG.use_amp and self.autocast_dtype is not None:\n",
    "                with autocast(dtype=self.autocast_dtype):\n",
    "                    logits = self.model(x)\n",
    "                    loss = self.criterion(logits, y)\n",
    "            else:\n",
    "                logits = self.model(x)\n",
    "                loss = self.criterion(logits, y)\n",
    "\n",
    "            # backward + step + EMA \n",
    "            if CFG.use_amp and AMP_DTYPE == \"bf16\":\n",
    "                self.scaler.scale(loss).backward()\n",
    "                if CFG.clip_grad_norm:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(), CFG.clip_grad_norm)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "\n",
    "                # EMA update with step \n",
    "                self._update_ema(self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            elif CFG.use_amp and AMP_DTYPE == \"bf16\":\n",
    "                loss.backward()\n",
    "                if CFG.clip_grad_norm:\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(), CFG.clip_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # EMA update with step \n",
    "                self._update_ema(self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            else:  # fp32\n",
    "                loss.backward()\n",
    "                if CFG.clip_grad_norm:\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(), CFG.clip_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # EMA update with step \n",
    "                self._update_ema(self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            running += loss.item()\n",
    "\n",
    "            if is_main_process() and (it+1) % 20 == 0:\n",
    "                dt = time() - t0\n",
    "                ips = (it+1) * CFG.batch_size / max(dt, 1e-6)\n",
    "                iterator.set_postfix(lr=f\"{lr:.2e}\", loss=f\"{loss.item():.4f}\", ips=f\"{ips:.1f} it/s\")\n",
    "\n",
    "        avg_loss = torch.tensor([running / max(1, len(self.train_loader))], device=self.device)\n",
    "        if self.is_distributed:\n",
    "            dist.all_reduce(avg_loss, op=dist.ReduceOp.AVG)\n",
    "        return avg_loss.item()\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        net = self.ema if getattr(self, \"ema\", None) is not None else (\n",
    "            self.model.module if isinstance(self.model, DDP) else self.model\n",
    "        )\n",
    "        net.eval()\n",
    "    \n",
    "        tot = 0.0\n",
    "        probs_all, tgts_all, sids_all = [], [], []\n",
    "    \n",
    "        for batch in self.val_loader:\n",
    "            # unpack (x,y,sid)\n",
    "            x, y, sid = batch\n",
    "            x = self._cast_input(x.to(self.device, non_blocking=True))\n",
    "            y = y.to(self.device, non_blocking=True)\n",
    "    \n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                logits = net(x)\n",
    "                loss = self.criterion(logits, y)\n",
    "            tot += float(loss.item())\n",
    "    \n",
    "            probs = torch.sigmoid(logits.float())\n",
    "            probs_all.append(probs.cpu().numpy())\n",
    "            tgts_all.append(y.float().cpu().numpy())\n",
    "            sids_all.extend(list(sid))\n",
    "    \n",
    "        # local stacks\n",
    "        y_prob = np.concatenate(probs_all, axis=0)\n",
    "        y_true = np.concatenate(tgts_all, axis=0)\n",
    "        sids   = np.array(sids_all)\n",
    "    \n",
    "        # clean\n",
    "        y_prob = np.clip(y_prob, 1e-6, 1-1e-6)\n",
    "        finite_mask = np.isfinite(y_prob).all(axis=1) & np.isfinite(y_true).all(axis=1)\n",
    "        y_prob, y_true, sids = y_prob[finite_mask], y_true[finite_mask], sids[finite_mask]\n",
    "    \n",
    "        # gather (no padding)\n",
    "        if self.is_distributed:\n",
    "            local = {\"prob\": y_prob, \"true\": y_true, \"sid\": sids}\n",
    "            gathered = [None] * self.world\n",
    "            dist.all_gather_object(gathered, local)\n",
    "            y_prob = np.concatenate([g[\"prob\"] for g in gathered if g is not None], axis=0)\n",
    "            y_true = np.concatenate([g[\"true\"] for g in gathered if g is not None], axis=0)\n",
    "            sids   = np.concatenate([g[\"sid\"]  for g in gathered if g is not None], axis=0)\n",
    "    \n",
    "        # metrics\n",
    "        aucs = {}\n",
    "        for j, name in enumerate(LABEL_COLS):\n",
    "            yi, pi = y_true[:, j], y_prob[:, j]\n",
    "            m = np.isfinite(yi) & np.isfinite(pi)\n",
    "            yi, pi = yi[m], pi[m]\n",
    "            aucs[name] = roc_auc_score(yi, pi) if np.unique(yi).size >= 2 else np.nan\n",
    "    \n",
    "        wauc = comp_weighted_auc(aucs)\n",
    "        va_loss = tot / max(1, len(self.val_loader))\n",
    "    \n",
    "        # SAVE fold-level OOF chunk (VAL predictions for this fold) on main process\n",
    "        if is_main_process():\n",
    "            df_pred = pd.DataFrame({\"SeriesInstanceUID\": sids})\n",
    "            for j, name in enumerate(LABEL_COLS):\n",
    "                df_pred[name] = y_prob[:, j]\n",
    "            # Optional: include targets for analysis\n",
    "            for j, name in enumerate(LABEL_COLS):\n",
    "                df_pred[f\"{name}_target\"] = y_true[:, j]\n",
    "            oof_path = os.path.join(self.fold_dir, f\"oof_fold{self.fold}_seed{CFG.seed}.csv\")\n",
    "            df_pred.to_csv(oof_path, index=False)\n",
    "    \n",
    "        return va_loss, wauc, aucs, y_prob  # (kept same return shape)\n",
    "\n",
    "    \n",
    "    def fit(self):\n",
    "        best_state = None\n",
    "        no_improve = 0\n",
    "        patience = 7\n",
    "    \n",
    "        for epoch in range(CFG.epochs):\n",
    "            tr_loss = self.one_epoch(epoch)\n",
    "            va_loss, wauc, aucs, _ = self.validate()\n",
    "            ap = aucs[LABEL_COLS[ANEURYSM_PRESENT_IDX]]\n",
    "    \n",
    "            if is_main_process():\n",
    "                print(f\"[{epoch+1:02d}/{CFG.epochs}] tr={tr_loss:.4f}  va={va_loss:.4f}  wAUC={wauc:.5f}  AneurysmPresent={ap:.5f}\")\n",
    "                \n",
    "            if wauc > self.best_auc:\n",
    "                self.best_auc = wauc\n",
    "                no_improve = 0\n",
    "                # Save EMA as the best snapshot\n",
    "                state = copy.deepcopy(self.ema.state_dict())\n",
    "                best_state = state  # keep a local copy for end-of-training reload\n",
    "                if is_main_process():\n",
    "                    torch.save(\n",
    "                    {\"state_dict\": state, \"cfg\": cfg_to_dict(CFG), \"best_wAUC\": float(wauc),\n",
    "                     \"fold\": int(self.fold), \"is_ema\": True},\n",
    "                    os.path.join(self.fold_dir, \"best_ema.pth\")\n",
    "                    )\n",
    "                    # raw/live weights (optional)\n",
    "                    live = (self.model.module if isinstance(self.model, DDP) else self.model).state_dict()\n",
    "                    torch.save(\n",
    "                        {\"state_dict\": live, \"cfg\": cfg_to_dict(CFG), \"best_wAUC\": float(wauc),\n",
    "                         \"fold\": int(self.fold), \"is_ema\": False},\n",
    "                        os.path.join(self.fold_dir, \"best_raw.pth\")\n",
    "                    )\n",
    "                    # metrics sidecar\n",
    "                    with open(os.path.join(self.fold_dir, \"metrics.json\"), \"w\") as f:\n",
    "                        json.dump({\"best_wAUC\": float(wauc), \"epoch\": int(epoch), \"fold\": int(self.fold)}, f, indent=2)\n",
    "            else:\n",
    "                no_improve += 1\n",
    "    \n",
    "            if no_improve >= patience:\n",
    "                if is_main_process(): \n",
    "                    print(\"Early stopping.\")\n",
    "                break\n",
    "    \n",
    "        # Load EMA-best back into the active model so further eval/infer use it\n",
    "        if best_state is not None:\n",
    "            target = self.model.module if isinstance(self.model, DDP) else self.model\n",
    "            target.load_state_dict(best_state, strict=False)\n",
    "    \n",
    "        if is_main_process():\n",
    "            print(\"Best wAUC:\", self.best_auc)\n",
    "    \n",
    "        return self.best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HPO with Optuna (single-GPU, subset) \n",
    "# import os, math, json, random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import optuna\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# HPO_MAX_TRIALS   = 20            # ~20 trials is plenty for a quick sweep\n",
    "# HPO_EPOCHS       = 7             # short runs\n",
    "# HPO_PATIENCE     = 3\n",
    "# SUBSET_FRACTION  = 0.20          # use 20% of train for proxy\n",
    "# HPO_FOLD_IDX     = 0             # keep consistent with your main setup\n",
    "# HPO_SEED         = 123\n",
    "\n",
    "# def set_hpo_cfg(trial):\n",
    "#     CFG.lr            = trial.suggest_float(\"lr\", 1.0e-4, 2.5e-4, log=True)\n",
    "#     CFG.weight_decay  = trial.suggest_float(\"weight_decay\", 0.02, 0.08)\n",
    "#     CFG.warmup_epochs = trial.suggest_float(\"warmup_epochs\", 0, 1)\n",
    "#     CFG.min_lr        = trial.suggest_float(\"min_lr\", 2e-6, 8e-6, log=True)\n",
    "#     CFG.label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.05)\n",
    "#     # optional:\n",
    "#     # CFG.pos_weight   = trial.suggest_categorical(\"pos_weight\", [1.0, 1.5])\n",
    "#     CFG.ema_decay    = trial.suggest_float(\"ema_decay\", 0.9990, 0.9999)\n",
    "#     # training budget for proxy\n",
    "#     CFG.epochs        = HPO_EPOCHS\n",
    "#     return CFG\n",
    "\n",
    "# def filter_existing_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Keep rows whose series exists either as a DICOM folder (if series_root is valid),\n",
    "#     or as a cached .npy in any shard if series_root is missing/unavailable.\n",
    "#     \"\"\"\n",
    "#     # If we have a real series_root, use it\n",
    "#     if getattr(CFG, \"series_root\", None) and os.path.isdir(CFG.series_root):\n",
    "#         exists = df[\"SeriesInstanceUID\"].astype(str).apply(\n",
    "#             lambda u: os.path.isdir(os.path.join(CFG.series_root, u))\n",
    "#         )\n",
    "#         return df[exists].reset_index(drop=True)\n",
    "\n",
    "#     # Otherwise, fall back to cache presence\n",
    "#     shard_roots = discover_shard_roots()\n",
    "\n",
    "#     def has_cache(sid: str) -> bool:\n",
    "#         fname = f\"{sid}_{CFG.img_size}.npy\"\n",
    "#         for root in shard_roots:\n",
    "#             if os.path.exists(os.path.join(root, fname)):\n",
    "#                 return True\n",
    "#         return False\n",
    "\n",
    "#     exists = df[\"SeriesInstanceUID\"].astype(str).apply(has_cache)\n",
    "#     return df[exists].reset_index(drop=True)\n",
    "\n",
    "# def build_subset_fold(df_full: pd.DataFrame, fold_idx: int, frac: float, seed: int):\n",
    "#     # Filter to only series we can actually use (raw OR cache)\n",
    "#     df = filter_existing_series(df_full.copy())\n",
    "\n",
    "#     # Recreate the fold assignment (same as training)\n",
    "#     from sklearn.model_selection import StratifiedKFold\n",
    "#     skf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
    "#     df[\"fold\"] = -1\n",
    "#     for f_i, (_, val_idx) in enumerate(skf.split(df, df[\"Aneurysm Present\"].astype(int))):\n",
    "#         df.loc[val_idx, \"fold\"] = f_i\n",
    "\n",
    "#     train_df = df[df[\"fold\"] != fold_idx].reset_index(drop=True)\n",
    "#     val_df   = df[df[\"fold\"] == fold_idx].reset_index(drop=True)\n",
    "\n",
    "#     # Subsample for fast HPO\n",
    "#     train_df = train_df.sample(frac=frac, random_state=seed).reset_index(drop=True)\n",
    "#     val_df   = val_df.sample(frac=min(1.0, frac), random_state=seed).reset_index(drop=True)  # keep similar scale\n",
    "\n",
    "#     return train_df, val_df\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     # hyperparams -> CFG\n",
    "#     set_hpo_cfg(trial)\n",
    "\n",
    "#     # single-GPU mode (disable DDP)\n",
    "#     os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "#     os.environ[\"RANK\"] = \"0\"\n",
    "#     os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "\n",
    "#     # subset fold\n",
    "#     df_full = pd.read_csv(CFG.train_csv)\n",
    "#     train_df, val_df = build_subset_fold(df_full, HPO_FOLD_IDX, SUBSET_FRACTION, HPO_SEED)\n",
    "\n",
    "#     # build loaders reusing dataset/preproc/cache\n",
    "#     shard_roots = discover_shard_roots()\n",
    "#     find_cached_path_fn = make_find_cached_path(shard_roots)\n",
    "#     preproc = DICOMPreprocessor(target_shape=(CFG.in_chans, CFG.img_size, CFG.img_size))\n",
    "\n",
    "#     train_ds = RSNADataset(train_df, CFG.series_root, preproc, find_cached_path_fn)\n",
    "#     val_ds   = RSNADataset(val_df,   CFG.series_root, preproc, find_cached_path_fn)\n",
    "\n",
    "#     train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,\n",
    "#                               num_workers=CFG.num_workers, pin_memory=False, drop_last=True)\n",
    "#     val_loader   = DataLoader(val_ds,   batch_size=max(1, CFG.batch_size//2), shuffle=False,\n",
    "#                               num_workers=CFG.num_workers, pin_memory=False)\n",
    "\n",
    "#     # trainer but with shorter patience and (optionally) no EMA saving spam\n",
    "#     trainer = Trainer(train_loader, val_loader, fold=HPO_FOLD_IDX)\n",
    "#     import time \n",
    "#     def fit_with_pruning():\n",
    "#         best = -1.0\n",
    "#         no_improve = 0\n",
    "#         for epoch in range(CFG.epochs):\n",
    "#             t0 = time.time()\n",
    "#             _ = trainer.one_epoch(epoch)\n",
    "#             va_loss, wauc, aucs, _ = trainer.validate()\n",
    "#             print(f\"[Trial {trial.number}] epoch {epoch+1}/{CFG.epochs} wAUC={wauc:.4f} (took {time.time()-t0:.1f}s)\")\n",
    "#             trial.report(wauc, step=epoch)\n",
    "#             if trial.should_prune():\n",
    "#                 print(f\"[Trial {trial.number}] pruned at epoch {epoch+1}\")\n",
    "#                 raise optuna.exceptions.TrialPruned()\n",
    "#             if wauc > best:\n",
    "#                 best, no_improve = wauc, 0\n",
    "#             else:\n",
    "#                 no_improve += 1\n",
    "#                 if no_improve >= HPO_PATIENCE:\n",
    "#                     break\n",
    "#         return best\n",
    "\n",
    "#     best_wauc = fit_with_pruning()\n",
    "#     return best_wauc\n",
    "\n",
    "# # Build pruner & sampler\n",
    "# pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource=2, reduction_factor=2, min_early_stopping_rate=0)\n",
    "# # alternative: optuna.pruners.MedianPruner(n_startup_trials=6, n_warmup_steps=1)\n",
    "\n",
    "# sampler = optuna.samplers.TPESampler(seed=HPO_SEED, multivariate=True, group=True)\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner)\n",
    "# study.optimize(objective, n_trials=HPO_MAX_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# print(\"Best trial:\", study.best_trial.number)\n",
    "# print(\"Best value (wAUC):\", study.best_value)\n",
    "# print(\"Best params:\", study.best_params)\n",
    "\n",
    "# # Apply best to CFG for a full re-train later\n",
    "# CFG.lr = study.best_params.get(\"lr\", CFG.lr)\n",
    "# CFG.weight_decay = study.best_params.get(\"weight_decay\", CFG.weight_decay)\n",
    "# CFG.warmup_epochs = study.best_params.get(\"warmup_epochs\", CFG.warmup_epochs)\n",
    "# CFG.min_lr = study.best_params.get(\"min_lr\", CFG.min_lr)\n",
    "# CFG.label_smoothing = study.best_params.get(\"label_smoothing\", CFG.label_smoothing)\n",
    "# # CFG.pos_weight = study.best_params.get(\"pos_weight\", CFG.pos_weight)\n",
    "# CFG.ema_decay  = study.best_params.get(\"ema_decay\", CFG.ema_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value (wAUC): 0.6191915125349632\n",
    "Best params: {'lr': 0.00019798869977282848, 'weight_decay': 0.020602267473080457, 'warmup_epochs': 0.03124208230906156, 'min_lr': 5.066102419292564e-06, 'label_smoothing': 0.0329264605941166, 'ema_decay': 0.9990402085327842}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:34:05.55325Z",
     "iopub.status.busy": "2025-09-17T13:34:05.553005Z",
     "iopub.status.idle": "2025-09-17T13:40:58.431366Z",
     "shell.execute_reply": "2025-09-17T13:40:58.426848Z",
     "shell.execute_reply.started": "2025-09-17T13:34:05.553226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device name: NVIDIA GeForce RTX 3070 Ti\n",
      "Found shard roots: 4\n",
      "   D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/cache\\SHARD_ID=0\\cache_u8_384_shard00\n",
      "   D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/cache\\SHARD_ID=1\\cache_u8_384_shard01\n",
      "   D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/cache\\SHARD_ID=2\\cache_u8_384_shard02\n",
      "   D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/cache\\SHARD_ID=3\\cache_u8_384_shard03\n",
      "[manifest] Found 4 manifest(s).\n",
      "[manifest] Loaded orig_depth for 4348 series.\n",
      "Sample cached path: D:/User Data/Downloads/rsna-intracranial-aneurysm-detection/cache\\SHARD_ID=3\\cache_u8_384_shard03\\1.2.826.0.1.3680043.8.498.10004044428023505108375152878107656647_384.npy\n",
      "World size: 1  |  Rank: 0  |  Local rank: 0\n",
      "Train: 3478 | Val: 870\n",
      "[single] world=1 rank=0 local_rank=0  |  Train: 3478  Val: 870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38230114aea9492fa3ce5b122bc4e981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/34] tr=0.3013  va=0.6350  wAUC=0.50130  AneurysmPresent=0.49044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b86ff9b49d438cba680ad68939f9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/34] tr=0.2226  va=0.4030  wAUC=0.54376  AneurysmPresent=0.55275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9019da2a7fdb4b3cb752ec12f5ae06b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/34] tr=0.2195  va=0.2438  wAUC=0.61181  AneurysmPresent=0.66120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c0b45346a94c2a93159c756ba98ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/34] tr=0.2145  va=0.2273  wAUC=0.66625  AneurysmPresent=0.68818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b421ad24ab45aea8ce76f77916fbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/34] tr=0.2130  va=0.2214  wAUC=0.70185  AneurysmPresent=0.71401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a6e7316ce4811a18c37d45f0743a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/34] tr=0.2117  va=0.2161  wAUC=0.71291  AneurysmPresent=0.72979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68821436a3f43bb9eed82986db2176f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/34] tr=0.2093  va=0.2125  wAUC=0.70734  AneurysmPresent=0.72029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b50c6929fd49fe8d840ca0b9e11d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/34] tr=0.2065  va=0.2077  wAUC=0.73376  AneurysmPresent=0.74863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5ea624383444ab9cb85908a8af4add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/34] tr=0.2052  va=0.2077  wAUC=0.73702  AneurysmPresent=0.75208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ae59875ae449239bdb7e6088093cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/34] tr=0.2016  va=0.2096  wAUC=0.73526  AneurysmPresent=0.74591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cd776ffe6e4c7db9716bdf7f2f3d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/34] tr=0.2022  va=0.2063  wAUC=0.74291  AneurysmPresent=0.75692\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fb914f9c65442893e6c06900a6bd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/34] tr=0.1993  va=0.2063  wAUC=0.74173  AneurysmPresent=0.75838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0529f3bbdaf4547af84dc7893220e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/34] tr=0.1971  va=0.2073  wAUC=0.74082  AneurysmPresent=0.75606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875e6408567f449dbcc8786cab86da01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/34] tr=0.1950  va=0.2046  wAUC=0.74757  AneurysmPresent=0.76301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be89b4f2cb4b4f358cd2acfdb9b68fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/34] tr=0.1906  va=0.2031  wAUC=0.75179  AneurysmPresent=0.76929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b76972f1ec54e11a3ff7a5b1cc249ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/34] tr=0.1869  va=0.2037  wAUC=0.75326  AneurysmPresent=0.77210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc2e389735d4f7ba65f680dbde80601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/34] tr=0.1806  va=0.2087  wAUC=0.75202  AneurysmPresent=0.77186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f06cb38b0774fd4a747080137745b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/34] tr=0.1752  va=0.2095  wAUC=0.74748  AneurysmPresent=0.76561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2050fd8f2a407184e43185b49f4c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/34] tr=0.1669  va=0.2205  wAUC=0.74443  AneurysmPresent=0.76623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649383d1a2c045549b3a790236cc19b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/34] tr=0.1623  va=0.2165  wAUC=0.73766  AneurysmPresent=0.75569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fc9581d03f48ee97a62ff318a142e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/34] tr=0.1514  va=0.2229  wAUC=0.73528  AneurysmPresent=0.75150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502d066d69fa4962853b3b41632867b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/34] tr=0.1413  va=0.2350  wAUC=0.72083  AneurysmPresent=0.73185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdf68757a684f2cab702d7fe88c0778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/34] tr=0.1344  va=0.2369  wAUC=0.72350  AneurysmPresent=0.73519\n",
      "Early stopping.\n",
      "Best wAUC: 0.7532575240948443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7532575240948443"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single-GPU local run (Windows/Jupyter safe)\n",
    "import os, platform, torch\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "CFG.num_workers = 0\n",
    "CFG.persistent_workers = False\n",
    "CFG.pin_memory = torch.cuda.is_available()\n",
    "\n",
    "# Force non-distributed mode\n",
    "def _setup_dist_dummy():\n",
    "    # local_rank, rank, world_size, is_distributed\n",
    "    return 0, 0, 1, False\n",
    "\n",
    "def _cleanup_dist_dummy():\n",
    "    pass\n",
    "\n",
    "# Override any previous distributed helpers\n",
    "setup_distributed = _setup_dist_dummy\n",
    "cleanup_distributed = _cleanup_dist_dummy\n",
    "\n",
    "# Optional: small perf knobs\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True  # speeds up convs with fixed input size\n",
    "\n",
    "# Train single process \n",
    "from __main__ import build_loaders, Trainer, CFG, seed_everything  # ensure these are defined above\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train_loader, val_loader, fold_idx, world, r, local = build_loaders(fold_idx=0)\n",
    "print(f\"[single] world={world} rank={r} local_rank={local}  |  \"\n",
    "      f\"Train: {len(train_loader.dataset)}  Val: {len(val_loader.dataset)}\")\n",
    "\n",
    "trainer = Trainer(train_loader, val_loader, fold=fold_idx)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-17T13:40:58.432368Z",
     "iopub.status.idle": "2025-09-17T13:40:58.432728Z",
     "shell.execute_reply": "2025-09-17T13:40:58.43256Z",
     "shell.execute_reply.started": "2025-09-17T13:40:58.432544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Inference + Kaggle Server \n",
    "# import os, gc, json, shutil, warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# import numpy as np\n",
    "# import polars as pl\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.cuda.amp import autocast\n",
    "# import timm\n",
    "\n",
    "# ID_COL = \"SeriesInstanceUID\"\n",
    "# TARGET_COLS = LABEL_COLS\n",
    "# NUM_CLASSES = len(TARGET_COLS)\n",
    "\n",
    "# class InferenceCFG:\n",
    "#     model_name: str = CFG.model_name              # must match training arch\n",
    "#     img_size:  int = CFG.img_size                 # 384 by default\n",
    "#     in_chans:  int = CFG.in_chans                 # 32\n",
    "#     num_classes: int = NUM_CLASSES\n",
    "#     out_dir: str = CFG.out_dir\n",
    "#     save_name: str = CFG.save_name\n",
    "#     seed: int = CFG.seed\n",
    "#     folds: List[int] = [0]                        # <- start with [0]; later: [0,1,2,3,4]\n",
    "#     use_amp: bool = True                          # AMP is fine for inference; set False if see NaNs\n",
    "#     channels_last: bool = True                    # match training memory format\n",
    "#     device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ICFG = InferenceCFG()\n",
    "\n",
    "# def _make_model_for_infer() -> nn.Module:\n",
    "#     m = timm.create_model(\n",
    "#         ICFG.model_name,\n",
    "#         in_chans=ICFG.in_chans,\n",
    "#         num_classes=ICFG.num_classes,\n",
    "#         img_size=ICFG.img_size,   # ignored by some convnets; fine for ViT/MaxViT\n",
    "#         pretrained=False\n",
    "#     )\n",
    "#     if ICFG.channels_last:\n",
    "#         m = m.to(memory_format=torch.channels_last)\n",
    "#     return m\n",
    "\n",
    "# def _load_fold_model(fold: int) -> nn.Module:\n",
    "#     fold_dir = Path(ICFG.out_dir) / f\"{ICFG.save_name}_seed{ICFG.seed}_fold{fold}\"\n",
    "#     ckpt_path = fold_dir / \"best_ema.pth\"  # saved in Trainer.fit()\n",
    "#     if not ckpt_path.exists():\n",
    "#         raise FileNotFoundError(f\"EMA checkpoint not found: {ckpt_path}\")\n",
    "#     ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "#     state = ckpt.get(\"state_dict\", ckpt.get(\"model\"))  # support older file shape\n",
    "\n",
    "#     model = _make_model_for_infer()\n",
    "#     model.load_state_dict(state, strict=True)\n",
    "#     model.to(ICFG.device).eval()\n",
    "#     return model\n",
    "\n",
    "# # Lazy global cache\n",
    "# _MODELS: Dict[int, nn.Module] = {}\n",
    "# def _ensure_models_loaded():\n",
    "#     global _MODELS\n",
    "#     if _MODELS:\n",
    "#         return\n",
    "#     for f in ICFG.folds:\n",
    "#         _MODELS[f] = _load_fold_model(f)\n",
    "#     # optional warmup\n",
    "#     with torch.no_grad():\n",
    "#         dummy = torch.randn(1, ICFG.in_chans, ICFG.img_size, ICFG.img_size, device=ICFG.device)\n",
    "#         for m in _MODELS.values():\n",
    "#             _ = m(dummy)\n",
    "\n",
    "# # DICOM → volume (32,H,W)\n",
    "# def _process_series(series_path: str, target_shape: Tuple[int,int,int]) -> np.ndarray:\n",
    "#     # Uses preprocessor defined earlier in the notebook\n",
    "#     pre = DICOMPreprocessor(target_shape=target_shape)\n",
    "#     vol = pre.process_series(series_path)   # expects uint8 or float; i normalized to u8@384 in cache, but here i recompute\n",
    "#     return vol\n",
    "\n",
    "# # Single forward (no TTA)\n",
    "# @torch.no_grad()\n",
    "# def _predict_single(model: nn.Module, vol_u8: np.ndarray) -> np.ndarray:\n",
    "#     # vol_u8: (32, H, W), uint8 or float in [0,255]; training scaled by /255\n",
    "#     x = torch.from_numpy(np.asarray(vol_u8)).to(torch.float32).div_(255.0).unsqueeze(0)  # (1,32,H,W)\n",
    "#     if ICFG.channels_last:\n",
    "#         x = x.contiguous(memory_format=torch.channels_last)\n",
    "#     x = x.to(ICFG.device, non_blocking=True)\n",
    "\n",
    "#     # AMP is okay for inference; set enabled=False if prefer fp32 only\n",
    "#     with autocast(enabled=ICFG.use_amp):\n",
    "#         logits = model(x)\n",
    "#     prob = torch.sigmoid(logits.float()).cpu().numpy().squeeze(0)  # (14,)\n",
    "#     prob = np.clip(prob, 1e-6, 1-1e-6)\n",
    "#     return prob\n",
    "\n",
    "# # Ensemble across folds\n",
    "# def _predict_ensemble(vol_u8: np.ndarray) -> np.ndarray:\n",
    "#     _ensure_models_loaded()\n",
    "#     preds = []\n",
    "#     for f, m in _MODELS.items():\n",
    "#         p = _predict_single(m, vol_u8)\n",
    "#         preds.append(p)\n",
    "#     return np.mean(np.stack(preds, axis=0), axis=0)  # equal-weight average\n",
    "\n",
    "\n",
    "# # Kaggle server predict(series_path) → Polars DF (NO ID column)\n",
    "# def _predict_inner(series_path: str) -> pl.DataFrame:\n",
    "#     # Build (32,img,img) volume exactly like train\n",
    "#     vol = _process_series(series_path, (ICFG.in_chans, ICFG.img_size, ICFG.img_size))\n",
    "#     pred = _predict_ensemble(vol)\n",
    "#     # Return Polars DataFrame with TARGET_COLS only (server handles the IDs)\n",
    "#     return pl.DataFrame([pred.tolist()], schema=TARGET_COLS)\n",
    "\n",
    "# def predict(series_path: str) -> pl.DataFrame:\n",
    "#     try:\n",
    "#         return _predict_inner(series_path)\n",
    "#     except Exception as e:\n",
    "#         # Conservative fallback (matches schema)\n",
    "#         fallback = [0.1] * NUM_CLASSES\n",
    "#         return pl.DataFrame([fallback], schema=TARGET_COLS)\n",
    "#     finally:\n",
    "#         # Required cleanup in Kaggle runtime between requests\n",
    "#         shared_dir = \"/kaggle/shared\"\n",
    "#         shutil.rmtree(shared_dir, ignore_errors=True)\n",
    "#         os.makedirs(shared_dir, exist_ok=True)\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import kaggle_evaluation.rsna_inference_server as rsna_eval\n",
    "\n",
    "# # Load once at startup (fast later)\n",
    "# _ensure_models_loaded()\n",
    "\n",
    "# server = rsna_eval.RSNAInferenceServer(predict)\n",
    "\n",
    "# if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "#     server.serve()\n",
    "# else:\n",
    "#     # Local gateway lets you test end-to-end and writes /kaggle/working/submission.parquet\n",
    "#     server.run_local_gateway()\n",
    "#     sub_df = pl.read_parquet(\"/kaggle/working/submission.parquet\")\n",
    "#     display(sub_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13762876,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 8285644,
     "sourceId": 13082129,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
